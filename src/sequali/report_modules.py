# Copyright (C) 2023 Leiden University Medical Center
# This file is part of sequali
#
# sequali is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# sequali is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with sequali.  If not, see <https://www.gnu.org/licenses/

import array
import collections
import dataclasses
import io
import math
import os
import sys
import time
import typing
from abc import ABC, abstractmethod
from collections import defaultdict
from pathlib import Path
from typing import (Any, Dict, Iterable, Iterator, List, Optional, Sequence,
                    Set, Tuple, Type)

import pygal  # type: ignore
import pygal.style  # type: ignore

from ._qc import A, C, G, N, T
from ._qc import (AdapterCounter, DedupEstimator, NanoStats, PerTileQuality,
                  QCMetrics, SequenceDuplication)
from ._qc import NUMBER_OF_NUCS, NUMBER_OF_PHREDS, PHRED_MAX
from ._version import __version__
from .adapters import Adapter
from .sequence_identification import DEFAULT_CONTAMINANTS_FILES, DEFAULT_K, \
    create_sequence_index, identify_sequence, reverse_complement
from .util import fasta_parser

SEQUALI_REPORT_CSS = Path(__file__).parent / "style" / "sequali_report.css"
SEQUALI_REPORT_CSS_CONTENT = SEQUALI_REPORT_CSS.read_text(encoding="utf-8")

PHRED_INDEX_TO_ERROR_RATE = [
    sum(10 ** (-p / 10) for p in range(start * 4, start * 4 + 4)) / 4
    for start in range(NUMBER_OF_PHREDS)
]
PHRED_INDEX_TO_PHRED = [-10 * math.log10(PHRED_INDEX_TO_ERROR_RATE[i])
                        for i in range(NUMBER_OF_PHREDS)]

QUALITY_SERIES_NAMES = (
    "0-3", "4-7", "8-11", "12-15", "16-19", "20-23", "24-27", "28-31",
    "32-35", "36-39", "40-43", ">=44")

# Colors below are generated by the generate_colors.py script.
# This is a diverging colormap RdBu (red to blue via white) from matplotlib
# setting the 12-15 category as center and the 0-3 and >=44 categories as
# extremes.
QUALITY_COLORS = [
    '#67001f',  # 0-3
    '#c94741',  # 4-7
    '#f7b799',  # 8-11
    '#f6f7f7',  # 12-15
    '#deebf2',  # 16-19
    '#c0dceb',  # 20-23
    '#98c8e0',  # 24-27
    '#68abd0',  # 28-31
    '#3e8cbf',  # 32-35
    '#2870b1',  # 36-39
    '#15508d',  # 40-43
    '#053061',  # >=44
]

COLOR_GREEN = "#33cc33"
COLOR_RED = "#ff0000"

QUALITY_DISTRIBUTION_STYLE = pygal.style.Style(colors=QUALITY_COLORS,
                                               font_family="sans-serif")
ONE_SERIE_STYLE = pygal.style.DefaultStyle(colors=("#33cc33",),  # Green
                                           font_family="sans-serif")
MULTIPLE_SERIES_STYLE = pygal.style.DefaultStyle(font_family="sans-serif")

DEFAULT_FRACTION_THRESHOLD = 0.0001
DEFAULT_MIN_THRESHOLD = 100
DEFAULT_MAX_THRESHOLD = sys.maxsize

COMMON_GRAPH_OPTIONS = dict(
    truncate_label=-1,
    width=1200,
    disable_xml_declaration=True,
    js=[],  # Script is globally downloaded once
)


def equidistant_ranges(length: int, parts: int) -> Iterator[Tuple[int, int]]:
    size = length // parts
    remainder = length % parts
    small_parts = parts - remainder
    start = 0
    for i in range(parts):
        part_size = size if i < small_parts else size + 1
        if part_size == 0:
            continue
        stop = start + part_size
        yield start, stop
        start = stop


def logarithmic_ranges(length: int, min_distance: int = 5):
    """
    Gives a squashed logarithmic range. It is not truly logarithmic as the
    minimum distance ensures that the lower units are more tightly packed.
    """
    # Use a scaling factor: this needs 400 units to reach the length of the
    # largest human chromosome. This will still fit on a graph once we reach
    # those sequencing sizes.
    scaling_factor = 250_000_000 ** (1 / 400)
    i = 0
    start = 0
    while True:
        stop = round(scaling_factor ** i)
        i += 1
        if stop >= start + min_distance:
            yield start, stop
            start = stop
            if stop >= length:
                return


def stringify_ranges(data_ranges: Iterable[Tuple[int, int]]):
    return [
        f"{start + 1}-{stop}" if start + 1 != stop else f"{start + 1}"
        for start, stop in data_ranges
    ]


def table_iterator(count_tables: array.ArrayType,
                   table_size: int) -> Iterator[memoryview]:
    table_view = memoryview(count_tables)
    for i in range(0, len(count_tables), table_size):
        yield table_view[i: i + table_size]


def aggregate_count_matrix(
        count_tables: array.ArrayType,
        data_ranges: Sequence[Tuple[int, int]],
        table_size: int) -> array.ArrayType:
    count_view = memoryview(count_tables)
    aggregated_matrix = array.array(
        "Q", bytes(8 * table_size * len(data_ranges)))
    ag_view = memoryview(aggregated_matrix)
    for cat_index, (start, stop) in enumerate(data_ranges):
        cat_offset = cat_index * table_size
        cat_view = ag_view[cat_offset:cat_offset + table_size]
        table_start = start * table_size
        table_stop = stop * table_size
        for i in range(table_size):
            cat_view[i] = sum(count_view[table_start + i: table_stop: table_size])
    return aggregated_matrix


def aggregate_base_tables(
        count_tables: array.ArrayType,
        data_ranges: Sequence[Tuple[int, int]],) -> array.ArrayType:
    return aggregate_count_matrix(count_tables, data_ranges, NUMBER_OF_NUCS)


def aggregate_phred_tables(
        count_tables: array.ArrayType,
        data_ranges: Sequence[Tuple[int, int]],) -> array.ArrayType:
    return aggregate_count_matrix(count_tables, data_ranges, NUMBER_OF_PHREDS)


def label_settings(x_labels: Sequence[str]) -> Dict[str, Any]:
    # Labels are ranges such as 1-5, 101-142 etc. This clutters the x axis
    # labeling so only use the first number. The values will be labelled
    # separately.
    simple_x_labels = [label.split("-")[0] for label in x_labels]
    if simple_x_labels and len(simple_x_labels[-1]) > 4:
        rotation = 30
    else:
        rotation = 0
    return dict(
        x_labels=simple_x_labels,
        x_labels_major_every=round(len(x_labels) / 30),
        x_label_rotation=rotation,
        show_minor_x_labels=False
    )


def label_values(values: Sequence[Any], labels: Sequence[Any]):
    if len(values) != len(labels):
        raise ValueError("labels and values should have the same length")
    return [{"value": value, "label": label} for value, label
            in zip(values, labels)]


@dataclasses.dataclass
class ReportModule(ABC):
    def __init__(self, *args, **kwargs):
        pass

    def from_dict(cls, d: Dict[str, Any]):
        return cls(**d)  # type: ignore

    def to_dict(self) -> Dict[str, Any]:
        return dataclasses.asdict(self)

    @abstractmethod
    def to_html(self) -> str:
        pass


@dataclasses.dataclass
class Meta(ReportModule):
    sequali_version: str
    report_generated: str
    filename: str
    filesize: int

    @classmethod
    def from_filepath(cls, filepath):
        filename = os.path.basename(filepath)
        try:
            filesize = os.stat(filepath).st_size
        except OSError:
            filesize = 0
        timestamp = time.time()
        time_struct = time.localtime(timestamp)
        report_generated = time.strftime("%Y-%m-%d %H:%M:%S%z", time_struct)
        return cls(__version__, report_generated, filename, filesize)

    def to_html(self) -> str:
        return f"""
            <p>Filename: <code>{self.filename}</code></p>
            <p>Filesize: {self.filesize / (1024 ** 3):.2f} GiB
            <p>Report generated on {self.report_generated}</p>
        """


@dataclasses.dataclass
class Summary(ReportModule):
    mean_length: float
    minimum_length: int
    maximum_length: int
    total_reads: int
    q20_reads: int
    total_bases: int
    q20_bases: int
    total_gc_bases: int
    total_n_bases: int

    def to_html(self) -> str:
        return f"""
            <h2>Summary</h2>
            <table>
            <tr><td>Mean length</td><td style="text-align:right;">
                {self.mean_length:,.2f}</td><td></td></tr>
            <tr><td>Length range (min-max)</td><td style="text-align:right;">
                {self.minimum_length:,}</td>
                <td style="text-align:right;">{self.maximum_length:,}</td></tr>
            <tr>
                <td>Total reads</td>
                <td style="text-align:right;">{self.total_reads:,}</td>
                <td></td></tr>
            <tr>
                <td> Q20 reads</td>
                <td style="text-align:right;">{self.q20_reads:,}</td>
                <td style="text-align:right;">
                    {self.q20_reads / max(self.total_reads, 1):.2%}
                </td>
            </tr>
            <tr><td>Total bases</td><td style="text-align:right;">
                {self.total_bases:,}</td><td></td></tr>
            <tr>
                <td>Total GC bases</td>
                <td style="text-align:right;">
                    {self.total_gc_bases:,}
                </td>
                <td style="text-align:right;">
                    {self.total_gc_bases / max(
                        self.total_bases - self.total_n_bases, 1):.2%}
                </td>
            </tr>
            <tr>
                <td>Q20 bases</td>
                <td style="text-align:right;">
                    {self.q20_bases:,}
                </td>
                <td style="text-align:right;">
                    {self.q20_bases / max(self.total_bases, 1):.2%}
                </td>
            </tr>
            </table>
        """


@dataclasses.dataclass
class SequenceLengthDistribution(ReportModule):
    length_ranges: List[str]
    counts: List[int]
    q1: int
    q5: int
    q10: int
    q25: int
    q50: int
    q75: int
    q90: int
    q95: int
    q99: int

    def plot(self) -> str:
        plot = pygal.Bar(
            title="Sequence length distribution",
            x_title="sequence length",
            y_title="number of reads",
            style=ONE_SERIE_STYLE,
            **label_settings(self.length_ranges),
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("Length", label_values(self.counts, self.length_ranges))
        return plot.render(is_unicode=True)

    def distribution_table(self):
        if len({self.q1, self.q5, self.q10, self.q25, self.q50,
                self.q75, self.q90, self.q95, self.q99}) == 1:
            return ""
        return f"""
            <table>
                <tr><td>N1</td><td style="text-align:right;">{self.q1:,}</td></tr>
                <tr><td>N5</td><td style="text-align:right;">{self.q5:,}</td></tr>
                <tr><td>N10</td><td style="text-align:right;">{self.q10:,}</td></tr>
                <tr><td>N25</td><td style="text-align:right;">{self.q25:,}</td></tr>
                <tr><td>N50</td><td style="text-align:right;">{self.q50:,}</td></tr>
                <tr><td>N75</td><td style="text-align:right;">{self.q75:,}</td></tr>
                <tr><td>N90</td><td style="text-align:right;">{self.q90:,}</td></tr>
                <tr><td>N95</td><td style="text-align:right;">{self.q95:,}</td></tr>
                <tr><td>N99</td><td style="text-align:right;">{self.q99:,}</td></tr>
            </table>
        """

    def to_html(self):
        return f"""
            <h2>Sequence length distribution</h2>
            <p>{self.distribution_table()}</p>
            <figure>{self.plot()}</figure>
        """

    @classmethod
    def from_base_count_tables(cls,
                               base_count_tables: array.ArrayType,
                               total_sequences: int,
                               data_ranges: Sequence[Tuple[int, int]]):
        max_length = len(base_count_tables) // NUMBER_OF_NUCS
        # use bytes constructor to initialize to 0
        sequence_lengths = array.array("Q", bytes(8 * (max_length + 1)))
        base_counts = array.array("Q", bytes(8 * (max_length + 1)))
        base_counts[0] = total_sequences  # all reads have at least 0 bases
        for i, table in enumerate(table_iterator(base_count_tables, NUMBER_OF_NUCS)):
            base_counts[i + 1] = sum(table)
        previous_count = 0
        for i in range(max_length, 0, -1):
            number_at_least = base_counts[i]
            sequence_lengths[i] = number_at_least - previous_count
            previous_count = number_at_least
        seqlength_view = memoryview(sequence_lengths)[1:]
        lengths = [sum(seqlength_view[start:stop]) for start, stop in
                   data_ranges]
        x_labels = stringify_ranges(data_ranges)
        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
        percentile_thresholds = [int(p * total_sequences / 100) for p in percentiles]
        thresh_iter = enumerate(percentile_thresholds)
        thresh_index, current_threshold = next(thresh_iter)
        accumulated_count = 0
        percentile_lengths = [0 for _ in percentiles]
        done = False
        for length, count in enumerate(sequence_lengths):
            while count > 0 and not done:
                remaining_threshold = current_threshold - accumulated_count
                if count > remaining_threshold:
                    accumulated_count += remaining_threshold
                    percentile_lengths[thresh_index] = length
                    count -= remaining_threshold
                    try:
                        thresh_index, current_threshold = next(thresh_iter)
                    except StopIteration:
                        done = True
                        break
                    continue
                break
            accumulated_count += count
            if done:
                break

        return cls(["0"] + x_labels, [sequence_lengths[0]] + lengths,
                   *percentile_lengths)


@dataclasses.dataclass
class PerPositionMeanQualityAndSpread(ReportModule):
    x_labels: List[str]
    percentiles: List[Tuple[str, List[float]]]

    def plot(self) -> str:
        plot = pygal.Line(
            title="Per position quality percentiles",
            show_dots=False,
            x_title="position",
            y_title="phred score",
            y_labels=list(range(0, 51, 10)),
            range=(0.0, 50.0),
            style=pygal.style.DefaultStyle(
                colors=["#000000"] * 12,
                font_family="sans-serif",
            ),
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        percentiles = dict(self.percentiles)
        plot.add("top 1%", label_values(percentiles["top 1%"], self.x_labels),
                 stroke_style={"dasharray": '1,2'})
        plot.add("top 5%", label_values(percentiles["top 5%"], self.x_labels),
                 stroke_style={"dasharray": '3,3'})
        plot.add("mean", label_values(percentiles["mean"], self.x_labels),
                 show_dots=True, dots_size=1)
        plot.add("bottom 5%", label_values(percentiles["bottom 5%"], self.x_labels),
                 stroke_style={"dasharray": '3,3'})
        plot.add("bottom 1%", label_values(percentiles["bottom 1%"], self.x_labels),
                 stroke_style={"dasharray": '1,2'})
        return plot.render(is_unicode=True)

    def to_html(self):
        return f"""
            <h2>Per position quality percentiles</h2>
            <p class="explanation">Shows the mean for all bases and the means
            of the lowest and
            highest percentiles to indicate the spread. Since the graph is
            based on the sampled categories, rather than exact phreds, it is
            an approximation.</p>
            <figure>{self.plot()}</figure>
        """

    @classmethod
    def from_phred_table_and_labels(cls, phred_tables: array.ArrayType, x_labels):
        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
        percentile_fractions = [i / 100 for i in percentiles]
        total_tables = len(phred_tables) // NUMBER_OF_PHREDS
        percentile_tables = [[0.0 for _ in range(total_tables)]
                             for _ in percentiles]
        reversed_percentile_tables = [[0.0 for _ in range(total_tables)]
                                      for _ in percentiles]
        mean = [0.0 for _ in range(total_tables)]
        for cat_index, table in enumerate(
                table_iterator(phred_tables, NUMBER_OF_PHREDS)):
            total = sum(table)
            if total == 0:
                continue
            total_error_rate = sum(
                PHRED_INDEX_TO_ERROR_RATE[i] * x for i, x in enumerate(table))
            percentile_thresholds = [int(f * total) for f in percentile_fractions]
            mean[cat_index] = -10 * math.log10(total_error_rate / total)
            accumulated_count = 0
            accumulated_errors = 0.0
            threshold_iter = enumerate(percentile_thresholds)
            thresh_index, current_threshold = next(threshold_iter)
            for phred_index, count in enumerate(table):
                while count > 0:
                    remaining_threshold = current_threshold - accumulated_count
                    if count > remaining_threshold:
                        accumulated_errors += (remaining_threshold *
                                               PHRED_INDEX_TO_ERROR_RATE[phred_index])
                        accumulated_count += remaining_threshold
                        if accumulated_count > 0:
                            percentile_tables[thresh_index][cat_index] = (
                                -10 * math.log10(
                                    accumulated_errors / accumulated_count))
                            reversed_percentile_tables[thresh_index][cat_index] = (
                                -10 * math.log10(
                                    (total_error_rate - accumulated_errors) /
                                    (total - accumulated_count)
                                )
                            )
                        count -= remaining_threshold
                        try:
                            thresh_index, current_threshold = next(threshold_iter)
                        except StopIteration:
                            # This will make sure the next cat_index is reached
                            # since 2 ** 65 will not be reached
                            thresh_index = sys.maxsize
                            current_threshold = 2**65
                        continue
                    break
                accumulated_count += count
                accumulated_errors += PHRED_INDEX_TO_ERROR_RATE[phred_index] * count
        graph_series = [
            ("bottom 1%", percentile_tables[0]),
            ("bottom 5%", percentile_tables[1]),
            ("bottom 10%", percentile_tables[2]),
            ("bottom 25%", percentile_tables[3]),
            ("bottom 50%", percentile_tables[4]),
            ("mean", mean),
            ("top 50%", reversed_percentile_tables[-5]),
            ("top 25%", reversed_percentile_tables[-4]),
            ("top 10%", reversed_percentile_tables[-3]),
            ("top 5%", reversed_percentile_tables[-2]),
            ("top 1%", reversed_percentile_tables[-1]),
        ]
        return cls(
            x_labels=x_labels,
            percentiles=graph_series
            )


@dataclasses.dataclass
class PerBaseQualityScoreDistribution(ReportModule):
    x_labels: Sequence[str]
    series: Sequence[Sequence[float]]

    @classmethod
    def from_phred_count_table_and_labels(
            cls, phred_tables: array.ArrayType, x_labels: Sequence[str]):
        total_tables = len(x_labels)
        quality_distribution = [
            [0.0 for _ in range(total_tables)]
            for _ in range(NUMBER_OF_PHREDS)
        ]
        for cat_index, table in enumerate(
                table_iterator(phred_tables, NUMBER_OF_PHREDS)):
            total_nucs = sum(table)
            if total_nucs == 0:
                continue
            for offset, phred_count in enumerate(table):
                if phred_count == 0:
                    continue
                nuc_fraction = phred_count / total_nucs
                quality_distribution[offset][cat_index] = nuc_fraction
        return cls(x_labels, quality_distribution)

    def plot(self) -> str:
        plot = pygal.StackedBar(
            title="Per base quality distribution",
            style=QUALITY_DISTRIBUTION_STYLE,
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="position",
            y_title="fraction",
            fill=True,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        for name, serie in zip(QUALITY_SERIES_NAMES, self.series):
            serie_filled = sum(serie) > 0.0
            plot.add(name, label_values(serie, self.x_labels),
                     show_dots=serie_filled)
        return plot.render(is_unicode=True)

    def to_html(self):
        return f"""
            <h2>Per position quality score distribution</h2>
            <figure>{self.plot()}</figure>
        """


@dataclasses.dataclass
class PerSequenceAverageQualityScores(ReportModule):
    average_quality_counts: Sequence[int]
    x_labels: Tuple[str, ...] = tuple(str(x) for x in range(PHRED_MAX + 1))

    def plot(self) -> str:
        maximum_score = 0
        for i, count in enumerate(self.average_quality_counts):
            if count > 0:
                maximum_score = i
        maximum_score = max(maximum_score + 2, 40)
        plot = pygal.Bar(
            title="Per sequence quality scores",
            x_labels=range(maximum_score + 1),
            x_labels_major_every=3,
            show_minor_x_labels=False,
            style=ONE_SERIE_STYLE,
            x_title="Phred score",
            y_title="Percentage of total",
            **COMMON_GRAPH_OPTIONS
        )
        total = sum(self.average_quality_counts)
        if total == 0:
            percentage_scores = [0.0 for _ in self.average_quality_counts]
        else:
            percentage_scores = [100 * score / total
                                 for score in self.average_quality_counts]

        plot.add("", percentage_scores[:maximum_score])
        return plot.render(is_unicode=True)

    def to_html(self) -> str:
        return f"""
            <h2>Per sequence average quality scores</h2>
            <figure>{self.plot()}</figure>
        """

    @classmethod
    def from_qc_metrics(cls, metrics: QCMetrics):
        return cls(list(metrics.phred_scores()))


@dataclasses.dataclass
class PerPositionBaseContent(ReportModule):
    x_labels: Sequence[str]
    A: Sequence[float]
    C: Sequence[float]
    G: Sequence[float]
    T: Sequence[float]

    def plot(self):
        style_class = pygal.style.Style
        green = COLOR_GREEN
        dark_green = "#228B22"  # ForestGreen
        blue = "#00BFFF"  # DeepSkyBlue
        dark_blue = "#1E90FF"  # DodgerBlue
        black = "#000000"
        style = style_class(
            colors=(green, dark_green, blue, dark_blue, black),
            font_family="sans-serif"
        )
        plot = pygal.StackedLine(
            title="Base content",
            style=style,
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="position",
            y_title="fraction",
            fill=True,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("G", label_values(self.G, self.x_labels))
        plot.add("C", label_values(self.C, self.x_labels))
        plot.add("A", label_values(self.A, self.x_labels))
        plot.add("T", label_values(self.T, self.x_labels))
        return plot.render(is_unicode=True)

    def to_html(self) -> str:
        return f"""
             <h2>Per position base content</h2>
             <figure>{self.plot()}</figure>
        """

    @classmethod
    def from_base_count_tables_and_labels(cls,
                                          base_count_tables: array.ArrayType,
                                          labels: Sequence[str]):
        total_tables = len(base_count_tables) // NUMBER_OF_NUCS
        base_fractions = [
            [0.0 for _ in range(total_tables)]
            for _ in range(NUMBER_OF_NUCS)
        ]
        for index, table in enumerate(
                table_iterator(base_count_tables, NUMBER_OF_NUCS)):
            total_bases = sum(table)
            n_bases = table[N]
            named_total = total_bases - n_bases
            if named_total == 0:
                continue
            base_fractions[A][index] = table[A] / named_total
            base_fractions[C][index] = table[C] / named_total
            base_fractions[G][index] = table[G] / named_total
            base_fractions[T][index] = table[T] / named_total
        return cls(
            labels,
            A=base_fractions[A],
            C=base_fractions[C],
            G=base_fractions[G],
            T=base_fractions[T]
        )


@dataclasses.dataclass
class PerPositionNContent(ReportModule):
    x_labels: Sequence[str]
    n_content: Sequence[float]

    @classmethod
    def from_base_count_tables_and_labels(
            cls, base_count_tables: array.ArrayType, labels: Sequence[str]):
        total_tables = len(base_count_tables) // NUMBER_OF_NUCS
        n_fractions = [0.0 for _ in range(total_tables)]
        for index, table in enumerate(
                table_iterator(base_count_tables, NUMBER_OF_NUCS)):
            total_bases = sum(table)
            if total_bases == 0:
                continue
            n_fractions[index] = table[N] / total_bases
        return cls(
            labels,
            n_fractions
        )

    def plot(self):
        plot = pygal.Bar(
            title="Per position N content",
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="position",
            y_title="fraction",
            fill=True,
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("N", label_values(self.n_content, self.x_labels))
        return plot.render(is_unicode=True)

    def to_html(self) -> str:
        return f"""
            <h2>Per position N content</h2>
            <figure>{self.plot()}</figure>
        """


@dataclasses.dataclass
class PerSequenceGCContent(ReportModule):
    gc_content_counts: Sequence[int]
    smoothened_gc_content_counts: Sequence[int]
    x_labels: Sequence[str] = tuple(str(x) for x in range(101))
    smoothened_x_labels: Sequence[str] = tuple(str(x) for x in range(0, 101, 2))

    def plot(self):
        plot = pygal.Bar(
            title="Per sequence GC content",
            x_labels=self.x_labels,
            x_labels_major_every=3,
            show_minor_x_labels=False,
            x_title="GC %",
            y_title="number of reads",
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("", self.gc_content_counts)
        return plot.render(is_unicode=True)

    def smoothened_plot(self):
        plot = pygal.Line(
            title="Per sequence GC content (smoothened)",
            x_labels=self.smoothened_x_labels,
            x_labels_major_every=3,
            show_minor_x_labels=False,
            x_title="GC %",
            y_title="number of reads",
            interpolate="cubic",
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("", self.smoothened_gc_content_counts)
        return plot.render(is_unicode=True)

    def to_html(self) -> str:
        return f"""
            <h2>Per sequence GC content</h2>
            <p class="explanation">
            For short reads with fixed size (i.e. Illumina) the plot will
            look very spiky due to the GC content calculation: GC bases / all
            bases. For read lengths of 151, both 75 and 76 GC bases lead to a
            percentage of 50% (rounded) and 72 and 73 GC bases leads to 48%
            (rounded). Only 74 GC bases leads to 49%. As a result the
            even categories will be twice as high, which creates a spike. The
            smoothened plot is provided to give a clearer picture in this case.
            </p>
            <figure>{self.plot()}</figure>
            <figure>{self.smoothened_plot()}</figure>
        """

    @classmethod
    def from_qc_metrics(cls, metrics: QCMetrics):
        gc_content = list(metrics.gc_content())
        smoothened_gc_content = []
        gc_content_iter = iter(gc_content)
        for i in range(50):
            smoothened_gc_content.append(next(gc_content_iter) + next(gc_content_iter))
        # Append the last 100% category.
        smoothened_gc_content.append(next(gc_content_iter))
        return cls(gc_content, smoothened_gc_content)


@dataclasses.dataclass
class AdapterContent(ReportModule):
    x_labels: Sequence[str]
    adapter_content: Sequence[Tuple[str, Sequence[float]]]

    def plot(self):
        plot = pygal.Line(
            title="Adapter content (%)",
            range=(0.0, 100.0),
            x_title="position",
            y_title="%",
            legend_at_bottom=True,
            legend_at_bottom_columns=1,
            truncate_legend=-1,
            style=MULTIPLE_SERIES_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        adapter_content = [(label, content) for label, content in
                           self.adapter_content if content and max(content) >= 0.1]
        adapter_content.sort(key=lambda x: max(x[1]),
                             reverse=True)
        for label, content in adapter_content:
            plot.add(label, label_values(content, self.x_labels))
        return plot.render(is_unicode=True)

    def to_html(self):
        return f"""
            <h2>Adapter content</h2>
            <p class="explanation">Only adapters that are present more than 0.1%
            are shown. Given the 12&#8239;bp
            length of the sequences used to estimate the content, values below this
            threshold are problably false positives. The legend is sorted from
            most frequent to least frequent.</p>
            <p class="explanation">For nanopore the the adapter mix (AMX) and
            ligation kit have overlapping adapter sequences and are therefore
            indistinguishable. Please consult the
            <a href="https://help.nanoporetech.com/en/articles/6632917-what-are-the-adapter-sequences-used-in-the-kits">
            nanopore documentation</a> for more information which adapters are
            used by your kit.</p>
            <p class="explanation">For illumina short reads, the last part of
            the graph will be flat as the 12&#8239;bp probes cannot be found in
            the last 11 base pairs.
            <figure>{self.plot()}</figure>
        """  # noqa: E501

    @classmethod
    def from_adapter_counter_adapters_and_ranges(
            cls, adapter_counter: AdapterCounter, adapters: Sequence[Adapter],
            data_ranges: Sequence[Tuple[int, int]]):

        def accumulate_counts(counts: Iterable[int]) -> List[int]:
            total = 0
            accumulated_counts = []
            for count in counts:
                total += count
                accumulated_counts.append(total)
            return accumulated_counts

        all_adapters = []
        sequence_to_adapter = {adapter.sequence: adapter for adapter in adapters}
        adapter_names = [adapter.name for adapter in adapters]
        total_sequences = adapter_counter.number_of_sequences
        for adapter_sequence, countarray in adapter_counter.get_counts():
            adapter = sequence_to_adapter[adapter_sequence]
            adapter_counts = [sum(countarray[start:stop])
                              for start, stop in data_ranges]
            if adapter.sequence_position == "end":
                accumulated_counts = accumulate_counts(adapter_counts)
            else:
                # Reverse the counts, accumulate them and reverse again for
                # adapters at the front.
                accumulated_counts = list(reversed(
                    accumulate_counts(reversed(adapter_counts))))
            all_adapters.append([count * 100 / total_sequences
                                 for count in accumulated_counts])
        return cls(stringify_ranges(data_ranges),
                   list(zip(adapter_names, all_adapters)))


@dataclasses.dataclass
class PerTileQualityReport(ReportModule):
    x_labels: Sequence[str]
    normalized_per_tile_averages: Sequence[Tuple[str, Sequence[float]]]
    tiles_2x_errors: Sequence[str]
    tiles_10x_errors: Sequence[str]
    skipped_reason: Optional[str]

    @classmethod
    def from_per_tile_quality_and_ranges(
            cls, ptq: PerTileQuality, data_ranges: Sequence[Tuple[int, int]]):
        if ptq.skipped_reason:
            return cls([], [], [], [], ptq.skipped_reason)
        average_phreds = []
        per_category_totals = [0.0 for i in range(len(data_ranges))]
        tile_counts = ptq.get_tile_counts()
        for tile, summed_errors, counts in tile_counts:
            range_averages = [
                sum(summed_errors[start:stop]) / sum(counts[start:stop])
                for start, stop in data_ranges]
            range_phreds = []
            for i, average in enumerate(range_averages):
                phred = -10 * math.log10(average)
                range_phreds.append(phred)
                # Averaging phreds takes geometric mean.
                per_category_totals[i] += phred
            average_phreds.append((tile, range_phreds))
        number_of_tiles = len(tile_counts)
        averages_per_category = [total / number_of_tiles
                                 for total in per_category_totals]
        normalized_averages = []
        tiles_2x_errors = []
        tiles_10x_errors = []
        for tile, tile_phreds in average_phreds:
            if not tile_phreds:
                continue
            normalized_tile_phreds = [
                tile_phred - average
                for tile_phred, average in
                zip(tile_phreds, averages_per_category)
            ]
            lowest_phred = min(normalized_tile_phreds)
            if lowest_phred <= -10.0:
                tiles_10x_errors.append(str(tile))
            elif lowest_phred <= -3.0:
                tiles_2x_errors.append(str(tile))
            normalized_averages.append((str(tile), normalized_tile_phreds))
        return cls(
            x_labels=stringify_ranges(data_ranges),
            normalized_per_tile_averages=normalized_averages,
            tiles_2x_errors=tiles_2x_errors,
            tiles_10x_errors=tiles_10x_errors,
            skipped_reason=ptq.skipped_reason,
        )

    def plot(self):
        style_colors = MULTIPLE_SERIES_STYLE.colors
        red = "#FF0000"
        yellow = "#FFD700"  # actually 'Gold' which is darker and more visible.
        style = pygal.style.Style(
            colors=(yellow, red) + style_colors,
            font_family="sans-serif",
        )
        scatter_plot = pygal.Line(
            title="Deviation from geometric mean in phred units.",
            x_title="position",
            stroke=False,
            style=style,
            y_title="Normalized phred",
            truncate_legend=-1,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )

        def add_horizontal_line(name, position):
            scatter_plot.add(name,
                             [position for _ in range(len(self.x_labels))],
                             show_dots=False, stroke=True, )

        add_horizontal_line("2 times more errors", -3)
        add_horizontal_line("10 times more errors", -10)

        min_phred = -10.0
        max_phred = 0.0
        for tile, tile_phreds in self.normalized_per_tile_averages:
            min_phred = min(min_phred, *tile_phreds)
            max_phred = max(max_phred, *tile_phreds)
            scatter_plot.range = (min_phred - 1, max_phred + 1)
            if min(tile_phreds) > -3 and max(tile_phreds) < 3:
                continue
            cleaned_phreds = [{'value': phred, 'label': label}
                              if (phred > 3 or phred < -3) else None
                              for phred, label in
                              zip(tile_phreds, self.x_labels)]
            scatter_plot.add(str(tile), cleaned_phreds)

        return scatter_plot.render(is_unicode=True)

    def to_html(self) -> str:
        header = "<h2>Per tile quality</h2>"
        if self.skipped_reason:
            return header + (f"Per tile quality skipped. Reason: "
                             f"{self.skipped_reason}.")
        return header + f"""
            <p class="explanation">
            This graph shows the deviation of each tile on each position from
            the geometric mean of all tiles at that position. The scale is
            expressed in phred units. -10 is 10 times more errors than the
            average.
            -3 is ~2 times more errors than the average. Only points that
            deviate more than 2 phred units from the average are shown. </p>
            <p>Tiles with more than 2 times the average error:
                {", ".join(self.tiles_2x_errors)}</p>
            <p>Tiles with more than 10 times the average error:
                {", ".join(self.tiles_10x_errors)}</p>
            <figure>{self.plot()}</figure>
        """


@dataclasses.dataclass
class DuplicationCounts(ReportModule):
    tracked_unique_sequences: int
    duplication_counts: Sequence[Tuple[int, int]]
    remaining_fraction: float
    estimated_duplication_fractions: Dict[str, float]

    def plot(self):
        plot = pygal.Bar(
            title="Duplication levels (%)",
            x_labels=list(self.estimated_duplication_fractions.keys()),
            x_title="Duplication counts",
            y_title="Percentage of total",
            x_label_rotation=30,
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("",
                 [100 * fraction for fraction in
                  self.estimated_duplication_fractions.values()])
        return plot.render(is_unicode=True)

    def to_html(self):
        first_part = f"""
        <p class="explanation">
        Every sequence is fingerprinted by skipping the first 64 bases and
        taking the first 8 bases after that, as well as getting the
        8 bases before the last 64 bases. This gives a small 16&#8239;bp
        sequence from two 8&#8239;bp stubs from the beginning and end.
        This sequence is hashed using the length divided by 64 as a seed,
        which results in the final fingerprint.
        This ensures that sequences that have very different lengths get
        different fingerprints. The 64&#8239;bp offset ensures that sequencing adapters
        at the beginning or end of the sequence are not taken into account. On
        short sequences, the offsets are proportionally shrunk.
        The 16&#8239;bp length of the sequence used as base for the hash limits
        the effect of sequencing errors, especially on long-read sequencing
        technologies.
        A subsample of the fingerprints is stored to
        estimate the duplication rate. See,
        <a href=https://www.usenix.org/system/files/conference/atc13/atc13-xie.pdf>
        the paper describing the methodology</a>.</p>
        <p>The subsample for this file consists of
        {self.tracked_unique_sequences:,} fingerprints.
        </p>
        <p>Estimated remaining sequences if deduplicated:
        {self.remaining_fraction:.2%}</p>
            """
        return f"""
            <h2>Duplication percentages</h2>
            {first_part}
            <figure>{self.plot()}</figure>
        """

    @staticmethod
    def estimated_counts_to_fractions(
            estimated_counts: Iterable[Tuple[int, int]]):
        named_slices = {
            "1": slice(1, 2),
            "2": slice(2, 3),
            "3": slice(3, 4),
            "4": slice(4, 5),
            "5": slice(5, 6),
            "6-10": slice(6, 11),
            "11-20": slice(11, 21),
            "21-30": slice(21, 31),
            "31-50": slice(31, 51),
            "51-100": slice(51, 101),
            "101-500": slice(101, 501),
            "501-1000": slice(501, 1001),
            "1001-5000": slice(1001, 5001),
            "5001-10000": slice(5001, 10_001),
            "10001-50000": slice(10_001, 50_001),
            "> 50000": slice(50_001, None),
        }
        count_array = array.array("Q", bytes(8 * 50002))
        for duplication, count in estimated_counts:
            if duplication > 50_000:
                count_array[50_001] += count * duplication
            else:
                count_array[duplication] = count * duplication
        total = max(sum(count_array), 1)
        aggregated_fractions = [
            sum(count_array[slc]) / total for slc in named_slices.values()
        ]
        return dict(zip(named_slices.keys(), aggregated_fractions))

    @staticmethod
    def deduplicated_fraction(duplication_counts: Dict[int, int]):
        total_sequences = sum(duplicates * count
                              for duplicates, count in
                              duplication_counts.items())
        unique_sequences = sum(duplication_counts.values())
        return unique_sequences / max(total_sequences, 1)

    @classmethod
    def from_dedup_estimator(cls, dedup_est: DedupEstimator):

        tracked_unique_sequences = dedup_est.tracked_sequences
        duplication_counts = dedup_est.duplication_counts()
        duplication_categories = collections.Counter(duplication_counts)
        estimated_duplication_fractions = cls.estimated_counts_to_fractions(
            duplication_categories.items())
        deduplicated_fraction = cls.deduplicated_fraction(
            duplication_categories)
        return cls(
            tracked_unique_sequences=tracked_unique_sequences,
            duplication_counts=sorted(duplication_categories.items()),
            estimated_duplication_fractions=estimated_duplication_fractions,
            remaining_fraction=deduplicated_fraction,
        )


class OverRepresentedSequence(typing.NamedTuple):
    count: int  # type: ignore
    fraction: float
    sequence: str
    revcomp_sequence: str
    most_matches: int
    max_matches: int
    best_match: str


@dataclasses.dataclass
class OverRepresentedSequences(ReportModule):
    overrepresented_sequences: List[OverRepresentedSequence]
    max_unique_fragments: int
    collected_fragments: int
    sample_every: int
    sequence_length: int
    total_fragments: int
    total_sequences: int
    sampled_sequences: int

    def to_dict(self) -> Dict[str, Any]:
        return {"overrepresented_sequences":
                [x._asdict() for x in self.overrepresented_sequences],
                "max_unique_fragments": self.max_unique_fragments,
                "sample_every": self.sample_every,
                "collected_fragments": self.collected_fragments,
                "sequence_length": self.sequence_length,
                "total_fragments": self.total_fragments,
                "total_sequences": self.total_sequences,
                "sampled_sequences": self.sampled_sequences}

    def from_dict(cls, d: Dict[str, List[Dict[str, Any]]]):
        overrepresented_sequences = d["overrepresented_sequences"]
        return cls([OverRepresentedSequence(**d)
                   for d in overrepresented_sequences],
                   max_unique_fragments=d["max_unique_fragments"],
                   collected_fragments=d["collected_fragments"],
                   sample_every=d["sample_every"],
                   sequence_length=d["sequence_length"],
                   total_fragments=d["total_fragments"],
                   total_sequences=d["total_sequences"],
                   sampled_sequences=d["sampled_sequences"])  # type: ignore

    def to_html(self) -> str:
        header = "<h2>Overrepresented sequences</h2>"
        if len(self.overrepresented_sequences) == 0:
            return header + "No overrepresented sequences."
        content = io.StringIO()
        content.write(header)
        content.write(
            f"""
            <p class="explanation">A subsample of the sequences is analysed
            Sequences are cut into fragments of up to 31&#8239;bp. Fragments
            are stored and counted. When the maximum amount of unique fragments
            is reached, only fragments that are already stored are counted. The
            rest of the fragments is ignored.
            Fragments are stored in their canonical representation. That is
            either the sequence or the reverse complement, whichever has
            the lowest sort order. Both representations are shown in the
            table.
            </p>
            <p class="explanation">
                The percentage shown is an estimate based on the number of
                occurences of the fragment in relation to the number of
                sampled sequences. This makes the assumption that a
                fragment only occurs once in each sequence.
            </p>
            <table>
            <tr>
                <td>Total sequences in file</td>
                <td style="text-align:right;">{self.total_sequences:,}</td>
            </tr>
            <tr>
                <td>Sampled sequences</td>
                <td style="text-align:right;">{self.sampled_sequences:,}</td>
            </tr>
            <tr>
                <td>Sampling rate</td>
                <td style="text-align:right;">1 in {self.sample_every}</td>
            </tr>
            <tr>
                <td>Total fragments sampled</td>
                <td style="text-align:right;">{self.total_fragments:,}</td>
            </tr>
            <tr>
                <td>Stored unique fragments</td>
                <td style="text-align:right;">{self.collected_fragments:,}</td>
            </tr>
            <tr>
                <td>Maximum unique fragments</td>
                <td style="text-align:right;">{self.max_unique_fragments:,}</td>
            </tr>
            <tr>
                <td>Fragment size</td>
                <td style="text-align:right;">{self.sequence_length}</td>
            </tr>
            </table>
            """
        )
        content.write("<table>")
        content.write("<tr><th>count</th><th>percentage</th>"
                      "<th>canonical sequence</th>"
                      "<th>reverse complemented sequence</th>"
                      "<th>kmers (matched/max)</th>"
                      "<th>best match</th></tr>")
        for item in self.overrepresented_sequences:
            content.write(
                f"""<tr><td style="text-align:right;">{item.count}</td>
                    <td style="text-align:right;">{item.fraction:.2%}</td>
                    <td style="text-align:center;font-family:monospace;">
                        {item.sequence}</td>
                    <td style="text-align:center;font-family:monospace;">
                        {item.revcomp_sequence}</td>
                    <td>({item.most_matches}/{item.max_matches})</td>
                    <td>{item.best_match}</td></tr>""")
        content.write("</table>")
        return content.getvalue()

    @classmethod
    def from_sequence_duplication(
            cls,
            seqdup: SequenceDuplication,
            fraction_threshold: float = DEFAULT_FRACTION_THRESHOLD,
            min_threshold: int = DEFAULT_MIN_THRESHOLD,
            max_threshold: int = DEFAULT_MAX_THRESHOLD,
    ):
        overrepresented_sequences = seqdup.overrepresented_sequences(
            fraction_threshold,
            min_threshold,
            max_threshold
        )
        if overrepresented_sequences:
            def contaminant_iterator():
                for file in DEFAULT_CONTAMINANTS_FILES:
                    yield from fasta_parser(file)

            sequence_index = create_sequence_index(contaminant_iterator(),
                                                   DEFAULT_K)
        else:  # Only spend time creating sequence index when its worth it.
            sequence_index = {}
        overrepresented_with_identification = [
            OverRepresentedSequence(
                count, fraction, sequence, reverse_complement(sequence),
                *identify_sequence(sequence, sequence_index))
            for count, fraction, sequence in overrepresented_sequences
        ]
        return cls(overrepresented_with_identification,
                   seqdup.max_unique_fragments,
                   seqdup.collected_unique_fragments,
                   seqdup.sample_every,
                   seqdup.fragment_length,
                   seqdup.total_fragments,
                   seqdup.number_of_sequences,
                   seqdup.sampled_sequences)


@dataclasses.dataclass
class NanoStatsReport(ReportModule):
    x_labels: List[str]
    time_bases: List[int]
    time_reads: List[int]
    time_active_channels: List[int]
    qual_percentages_over_time: List[List[float]]
    per_channel_bases: Dict[int, int]
    per_channel_quality: Dict[int, float]
    translocation_speed: List[int]
    skipped_reason: Optional[str] = None

    @staticmethod
    def seconds_to_hour_minute_notation(seconds: int):
        minutes = seconds // 60
        hours = minutes // 60
        minutes %= 60
        return f"{hours:02}:{minutes:02}"

    @classmethod
    def from_nanostats(cls, nanostats: NanoStats):
        if nanostats.skipped_reason:
            return cls(
                [],
                [],
                [],
                [],
                [],
                {},
                {},
                [],
                nanostats.skipped_reason
            )
        run_start_time = nanostats.minimum_time
        run_end_time = nanostats.maximum_time
        duration = run_end_time - run_start_time
        time_slots = 200
        time_per_slot = duration / time_slots
        time_interval_minutes = (math.ceil(time_per_slot) + 59) // 60
        time_interval = max(time_interval_minutes * 60, 1)
        # Use duration + 1 to avoid cases where time_interval x time_slots == duration
        # which causes the last slot to be missing.
        time_ranges = [(start, start + time_interval)
                       for start in range(0, duration + 1, time_interval)]
        time_slots = len(time_ranges)
        time_active_slots_sets: List[Set[int]] = [set() for _ in
                                                  range(time_slots)]
        time_bases = [0 for _ in range(time_slots)]
        time_reads = [0 for _ in range(time_slots)]
        time_qualities = [[0 for _ in range(12)] for _ in
                          range(time_slots)]
        per_channel_bases: Dict[int, int] = defaultdict(lambda: 0)
        per_channel_cumulative_error: Dict[int, float] = defaultdict(lambda: 0.0)
        translocation_speeds = [0] * 81
        for readinfo in nanostats.nano_info_iterator():
            relative_start_time = readinfo.start_time - run_start_time
            timeslot = relative_start_time // time_interval
            length = readinfo.length
            cumulative_error_rate = readinfo.cumulative_error_rate
            channel_id = readinfo.channel_id
            if length:
                phred = round(
                    -10 * math.log10(cumulative_error_rate / length))
            else:
                phred = 0
            phred_index = min(phred, 47) >> 2
            time_active_slots_sets[timeslot].add(channel_id)
            time_bases[timeslot] += length
            time_reads[timeslot] += 1
            time_qualities[timeslot][phred_index] += 1
            per_channel_bases[channel_id] += length
            per_channel_cumulative_error[channel_id] += cumulative_error_rate
            read_duration = readinfo.duration
            if read_duration:
                translocation_speed = min(round(length / read_duration), 800)
                translocation_speed //= 10
                translocation_speeds[translocation_speed] += 1
        per_channel_quality: Dict[int, float] = {}
        for channel, error_rate in per_channel_cumulative_error.items():
            total_bases = per_channel_bases[channel]
            if total_bases:
                phred_score = -10 * math.log10(error_rate / total_bases)
            else:
                phred_score = 0
            per_channel_quality[channel] = phred_score
        qual_percentages_over_time: List[List[float]] = [[] for _ in
                                                         range(12)]
        for quals in time_qualities:
            total = sum(quals)
            for i, q in enumerate(quals):
                qual_percentages_over_time[i].append(q / max(total, 1))
        time_active_slots = [len(s) for s in time_active_slots_sets]
        return cls(
            x_labels=[f"{cls.seconds_to_hour_minute_notation(start)}-"
                      f"{cls.seconds_to_hour_minute_notation(stop)}"
                      for start, stop in time_ranges],
            qual_percentages_over_time=qual_percentages_over_time,
            time_active_channels=time_active_slots,
            time_bases=time_bases,
            time_reads=time_reads,
            per_channel_bases=dict(sorted(per_channel_bases.items())),
            per_channel_quality=dict(sorted(per_channel_quality.items())),
            translocation_speed=translocation_speeds,
            skipped_reason=nanostats.skipped_reason
        )

    def time_bases_plot(self):
        plot = pygal.Bar(
            title="Base count over time",
            x_title="time(HH:MM)",
            y_title="base count",
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", label_values(self.time_bases, self.x_labels))
        return plot.render(is_unicode=True)

    def time_reads_plot(self):
        plot = pygal.Bar(
            title="Number of reads over time",
            x_title="time(HH:MM)",
            y_title="number of reads",
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", label_values(self.time_reads, self.x_labels))
        return plot.render(is_unicode=True)

    def time_active_channels_plot(self):
        plot = pygal.Bar(
            title="Active channels over time",
            x_title="time(HH:MM)",
            y_title="active channels",
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", label_values(self.time_active_channels, self.x_labels))
        return plot.render(is_unicode=True)

    def time_quality_distribution_plot(self):
        plot = pygal.StackedBar(
            title="Quality distribution over time",
            style=QUALITY_DISTRIBUTION_STYLE,
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="time(HH:MM)",
            y_title="fraction",
            fill=True,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        for name, serie in zip(QUALITY_SERIES_NAMES, self.qual_percentages_over_time):
            serie_filled = sum(serie) > 0.0
            plot.add(name, label_values(serie, self.x_labels),
                     show_dots=serie_filled)
        return plot.render(is_unicode=True)

    def channel_plot(self):
        plot = pygal.XY(
            title="Channel base yield and quality",
            dots_size=1,
            x_title="base yield (megabases)",
            y_title="quality (phred score)",
            stroke=False,
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS
        )
        serie = []
        for channel, base_yield in self.per_channel_bases.items():
            quality = self.per_channel_quality[channel]
            serie.append(dict(value=(base_yield/1_000_000, quality),
                              label=str(channel)))
        plot.add(None, serie)
        return plot.render(is_unicode=True)

    def translocation_section(self):
        transl_speeds = self.translocation_speed
        if sum(transl_speeds) == 0:
            return """
            <h2>translocation speeds</h2>
            Duration information not available.
            """
        too_slow = transl_speeds[:35] + [0] * 55
        too_fast = [0] * 45 + transl_speeds[45:]
        normal = [0] * 35 + transl_speeds[35:45] + [0] * 35
        total = sum(transl_speeds)
        within_bounds_frac = sum(normal) / total
        too_fast_frac = sum(too_fast) / total
        too_slow_frac = sum(too_slow) / total

        plot = pygal.Bar(
            title="Translocation speed distribution",
            x_title="Translocation_speed",
            y_title="active channels",
            style=pygal.style.DefaultStyle(
                # Use blue and red colors to accommodate colorblind people.
                colors=(QUALITY_COLORS[-3], QUALITY_COLORS[1], QUALITY_COLORS[1]),
                font_family="sans-serif",
            ),
            x_labels=[str(i) for i in range(0, 800, 10)] + [">800"],
            x_labels_major_every=10,
            show_minor_x_labels=False,
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("within bounds", normal)
        plot.add("too slow", too_slow)
        plot.add("too fast", too_fast)
        return f"""
        <h2>translocation speeds</h2>
        <p>Percentage of reads within accepted bounds: {within_bounds_frac:.2%}</p>
        <p>Percentage of reads that are too slow: {too_slow_frac:.2%}</p>
        <p>Percentage of reads that are too fast: {too_fast_frac:.2%}</p>
        <figure>{plot.render(is_unicode=True)}</figure>
        """

    def to_html(self) -> str:
        if self.skipped_reason:
            return f"""
            <h2>Nanopore time series</h2>
            Skipped: {self.skipped_reason}
            """
        return f"""
        <h2>Nanopore time series</h2>
        <h3>Base counts over time</h3>
        <figure>{self.time_bases_plot()}</figure>
        <h3>Read counts over time</h3>
        <figure>{self.time_reads_plot()}</figure>
        <h3>Active channels over time</h3>
        <figure>{self.time_active_channels_plot()}</figure>
        <h3>Quality distribution over time</h3>
        <figure>{self.time_quality_distribution_plot()}</figure>
        <h2>Per channel base yield versus quality<h2>
        <figure>{self.channel_plot()}</figure>
        {self.translocation_section()}
        """


NAME_TO_CLASS: Dict[str, Type[ReportModule]] = {
    "meta": Meta,
    "summary": Summary,
    "per_position_mean_quality_and_spread": PerPositionMeanQualityAndSpread,
    "per_position_quality_distribution": PerBaseQualityScoreDistribution,
    "sequence_length_distribution": SequenceLengthDistribution,
    "per_position_base_content": PerPositionBaseContent,
    "per_position_n_content": PerPositionNContent,
    "per_sequence_gc_content": PerSequenceGCContent,
    "per_sequence_quality_scores": PerSequenceAverageQualityScores,
    "adapter_content": AdapterContent,
    "per_tile_quality": PerTileQualityReport,
    "duplication_fractions": DuplicationCounts,
    "overrepresented_sequences": OverRepresentedSequences,
    "nanopore_metrics": NanoStatsReport,
}

CLASS_TO_NAME: Dict[Type[ReportModule], str] = {
    value: key for key, value in NAME_TO_CLASS.items()}


def report_modules_to_dict(report_modules: Iterable[ReportModule]):
    return {
        CLASS_TO_NAME[type(module)]: module.to_dict()
        for module in report_modules
    }


def dict_to_report_modules(d: Dict[str, Dict[str, Any]]) -> List[ReportModule]:
    return [NAME_TO_CLASS[name].from_dict(
                NAME_TO_CLASS[name], class_dict)  # type: ignore
            for name, class_dict in d.items()]


def write_html_report(report_modules: Iterable[ReportModule],
                      html: str,
                      filename: str):
    default_config = pygal.Config()
    pygal_script_uri = default_config.js[0].lstrip('/')
    with open(html, "wt", encoding="utf-8") as html_file:
        html_file.write(f"""
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <script src="https://{pygal_script_uri}"></script>
                <style>
                    {SEQUALI_REPORT_CSS_CONTENT}
                </style>
                <meta charset="utf-8">
                <title>{os.path.basename(filename)}: Sequali Report</title>
            </head>
            <header><p>
                Report created by sequali. Please visit the
                <a href="https://github.com/rhpvorderman/sequali">homepage</a>
                for bug reports and feature requests.
            </p></header>
            <h1>sequali report</h1>
        """)
        # size: {os.stat(filename).st_size / (1024 ** 3):.2f}GiB<br>
        for module in report_modules:
            html_file.write(module.to_html())
        html_file.write("</html>")


def qc_metrics_modules(metrics: QCMetrics,
                       data_ranges: Sequence[Tuple[int, int]]
                       ) -> List[ReportModule]:
    base_count_tables = metrics.base_count_table()
    phred_count_table = metrics.phred_count_table()
    x_labels = stringify_ranges(data_ranges)
    aggregrated_base_matrix = aggregate_count_matrix(
        base_count_tables, data_ranges, NUMBER_OF_NUCS)
    aggregated_phred_matrix = aggregate_count_matrix(
        phred_count_table, data_ranges, NUMBER_OF_PHREDS)
    summary_bases = aggregate_count_matrix(
        aggregrated_base_matrix,
        [(0, len(aggregrated_base_matrix) // NUMBER_OF_NUCS)], NUMBER_OF_NUCS)
    summary_phreds = aggregate_count_matrix(
        aggregated_phred_matrix,
        [(0, len(aggregated_phred_matrix) // NUMBER_OF_PHREDS)],
        NUMBER_OF_PHREDS)
    total_bases = sum(summary_bases)
    minimum_length = 0
    total_reads = metrics.number_of_reads
    q20_reads = sum(metrics.phred_scores()[20:])
    for table in table_iterator(base_count_tables, NUMBER_OF_NUCS):
        if sum(table) < total_reads:
            break
        minimum_length += 1
    total_gc_bases = summary_bases[C] + summary_bases[G]
    return [
        Summary(
            mean_length=total_bases / max(total_reads, 1),
            minimum_length=minimum_length,
            maximum_length=metrics.max_length,
            total_reads=total_reads,
            total_bases=total_bases,
            q20_bases=sum(summary_phreds[5:]),
            q20_reads=q20_reads,
            total_gc_bases=total_gc_bases,
            total_n_bases=summary_bases[N]),
        SequenceLengthDistribution.from_base_count_tables(
            base_count_tables, total_reads, data_ranges),
        PerBaseQualityScoreDistribution.from_phred_count_table_and_labels(
            aggregated_phred_matrix, x_labels),
        PerPositionMeanQualityAndSpread.from_phred_table_and_labels(
           aggregated_phred_matrix, x_labels),
        PerSequenceAverageQualityScores.from_qc_metrics(metrics),
        PerPositionBaseContent.from_base_count_tables_and_labels(
            aggregrated_base_matrix, x_labels),
        PerPositionNContent.from_base_count_tables_and_labels(
            aggregrated_base_matrix, x_labels),
        PerSequenceGCContent.from_qc_metrics(metrics),
    ]


def calculate_stats(
        filename: str,
        metrics: QCMetrics,
        adapter_counter: AdapterCounter,
        per_tile_quality: PerTileQuality,
        sequence_duplication: SequenceDuplication,
        dedup_estimator: DedupEstimator,
        nanostats: NanoStats,
        adapters: List[Adapter],
        graph_resolution: int = 200,
        fraction_threshold: float = DEFAULT_FRACTION_THRESHOLD,
        min_threshold: int = DEFAULT_MIN_THRESHOLD,
        max_threshold: int = DEFAULT_MAX_THRESHOLD,
) -> List[ReportModule]:
    max_length = metrics.max_length
    if max_length > 500:
        data_ranges = list(logarithmic_ranges(max_length))
    else:
        data_ranges = list(equidistant_ranges(max_length, graph_resolution))
    return [
        Meta.from_filepath(filename),
        *qc_metrics_modules(metrics, data_ranges),
        AdapterContent.from_adapter_counter_adapters_and_ranges(
            adapter_counter, adapters, data_ranges),
        PerTileQualityReport.from_per_tile_quality_and_ranges(
            per_tile_quality, data_ranges),
        DuplicationCounts.from_dedup_estimator(dedup_estimator),
        OverRepresentedSequences.from_sequence_duplication(
            sequence_duplication,
            fraction_threshold=fraction_threshold,
            min_threshold=min_threshold,
            max_threshold=max_threshold
        ),
        NanoStatsReport.from_nanostats(nanostats)
    ]
