# Copyright (C) 2023 Leiden University Medical Center
# This file is part of Sequali
#
# Sequali is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# Sequali is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Sequali.  If not, see <https://www.gnu.org/licenses/

import array
import collections
import dataclasses
import html
import io
import math
import os
import sys
import time
import typing
import xml.etree.ElementTree
from abc import ABC, abstractmethod
from collections import defaultdict
from pathlib import Path
from typing import (Any, Dict, Iterable, Iterator, List, Optional, Sequence,
                    Set, Tuple, Type)

import pygal  # type: ignore
import pygal.style  # type: ignore

from ._qc import A, C, G, N, T
from ._qc import (AdapterCounter, DedupEstimator,
                  INSERT_SIZE_MAX_ADAPTER_STORE_SIZE, InsertSizeMetrics,
                  NanoStats, PerTileQuality, QCMetrics, SequenceDuplication)
from ._qc import NUMBER_OF_NUCS, NUMBER_OF_PHREDS, PHRED_MAX
from ._version import __version__
from .adapters import Adapter
from .sequence_identification import (identify_sequence_builtin,
                                      reverse_complement)

SEQUALI_REPORT_CSS = Path(__file__).parent / "static" / "sequali_report.css"
SEQUALI_REPORT_CSS_CONTENT = SEQUALI_REPORT_CSS.read_text(encoding="utf-8")
SEQUALI_REPORT_JS = (Path(__file__).parent / "pygal.js" / "2.0.x" /
                     "pygal-tooltips.min.js")
SEQUALI_DOWNLOAD_SVG_JS = (Path(__file__).parent / "static" /
                           "svg_to_download_link.js")
SEQUALI_DOWNLOAD_SVG_JS_CONTENT = SEQUALI_DOWNLOAD_SVG_JS.read_text()
SEQUALI_REPORT_JS_CONTENT = SEQUALI_REPORT_JS.read_text()

PHRED_INDEX_TO_ERROR_RATE = [
    sum(10 ** (-p / 10) for p in range(start * 4, start * 4 + 4)) / 4
    for start in range(NUMBER_OF_PHREDS)
]
PHRED_INDEX_TO_PHRED = [-10 * math.log10(PHRED_INDEX_TO_ERROR_RATE[i])
                        for i in range(NUMBER_OF_PHREDS)]

QUALITY_SERIES_NAMES = (
    "0-3", "4-7", "8-11", "12-15", "16-19", "20-23", "24-27", "28-31",
    "32-35", "36-39", "40-43", ">=44")

# Colors below are generated by the generate_colors.py script.
# This is a diverging colormap RdBu (red to blue via white) from matplotlib
# setting the 12-15 category as center and the 0-3 and >=44 categories as
# extremes.
QUALITY_COLORS = [
    '#67001f',  # 0-3
    '#c94741',  # 4-7
    '#f7b799',  # 8-11
    '#f6f7f7',  # 12-15
    '#deebf2',  # 16-19
    '#c0dceb',  # 20-23
    '#98c8e0',  # 24-27
    '#68abd0',  # 28-31
    '#3e8cbf',  # 32-35
    '#2870b1',  # 36-39
    '#15508d',  # 40-43
    '#053061',  # >=44
]

COLOR_GREEN = "#33cc33"
COLOR_RED = "#ff0000"

COMMON_GRAPH_STYLE_OPTIONS = dict(
    font_family="sans-serif",
    label_font_size=14,
    major_label_font_size=14,
    value_label_font_size=14,
    title_font_size=18,
    legend_font_size=16,
)

QUALITY_DISTRIBUTION_STYLE = pygal.style.Style(colors=QUALITY_COLORS,
                                               **COMMON_GRAPH_STYLE_OPTIONS)
ONE_SERIE_STYLE = pygal.style.DefaultStyle(colors=("#33cc33",),  # Green
                                           **COMMON_GRAPH_STYLE_OPTIONS)
MULTIPLE_SERIES_STYLE = pygal.style.DefaultStyle(**COMMON_GRAPH_STYLE_OPTIONS)

DEFAULT_FRACTION_THRESHOLD = 0.0001
DEFAULT_MIN_THRESHOLD = 100
DEFAULT_MAX_THRESHOLD = sys.maxsize

COMMON_GRAPH_OPTIONS = dict(
    truncate_label=-1,
    width=1250,
    height=700,
    disable_xml_declaration=True,
    js=[],  # Script is globally downloaded once
)

READ1 = "Read 1"
READ2 = "Read 2"
NOT_OF_PAIR = None


def html_header(header: str, level: int, prefix: Optional[str] = None):
    if prefix is not None:
        header = f"{prefix}: {header}"
    html_id = header.lower().replace(" ", "-")
    return f"""
        <h{level} id="{html_id}">
            <a class="headerlink" href="#{html_id}">{header}</a>
        </h{level}>
    """


def create_toc(content: str):
    toc = io.StringIO()
    toc.write('<ul class="toc_list">')
    root = xml.etree.ElementTree.fromstring(content)
    current_level = 1
    for element in root.iter():  # type: xml.etree.ElementTree.Element
        tag = element.tag
        if tag.startswith("h") and len(tag) == 2 and tag[1].isdecimal():
            header_level = int(tag[1])
            reference_element = element.find("a")
            if reference_element is None:
                raise RuntimeError("Header format error. Can't create toc.")
            header = reference_element.text
            id = element.get("id")
            if header_level != current_level:
                if header_level > current_level:
                    for i in range(header_level - current_level):
                        toc.write('<li><ul class="toc_list">')
                else:
                    for i in range(current_level - header_level):
                        toc.write("</ul></li>")
                current_level = header_level
            toc.write(f'<li><a class="toclink" href="#{id}">{header}</a></li>')
    if current_level > 0:
        for i in range(current_level):
            toc.write("</ul>")
    return toc.getvalue()


def make_series_unique(svg: str) -> str:
    """
    Processes a PyGAL SVG to have unique serie IDs to fix
    https://github.com/Kozea/pygal/issues/563.
    """
    # TODO: Remove this once a fix is released in PyGAL and the PyGAL
    # requirement is updated.
    svg_element: xml.etree.ElementTree.Element = xml.etree.ElementTree.fromstring(svg)
    if not svg_element.tag.endswith("svg"):
        raise ValueError("This is not a svg xml")
    namespace = svg_element.tag.rstrip("svg")
    xml.etree.ElementTree.register_namespace("", namespace.strip("{}"))
    chart_id = svg_element.attrib["id"]
    for element in svg_element.iter():
        if not element.tag == f"{namespace}g":
            continue
        id = element.get("id")
        # Find activate-serie elements and append chart-id to their name to
        # make them unique.
        if id and id.startswith("activate-serie"):
            element.set("id", f"{id}-{chart_id}")
            continue

        # Individual series have a class attribute serie-<NUMBER> format.
        # Append the chart_id to make these unique.
        cls = element.get("class")
        if cls and "serie-" in cls:
            classifiers = cls.split()
            for classifier in classifiers:
                if classifier.startswith("serie-"):
                    to_replace = classifier
                    break
            else:  # No break
                raise RuntimeError("serie classifier should exist")
            classifiers.remove(to_replace)
            new_classifier = f"{to_replace}-{chart_id}"
            classifiers.append(new_classifier)
            element.set("class", " ".join(classifiers))
    return xml.etree.ElementTree.tostring(
        svg_element, encoding="unicode", method="xml")


def figurize_plot(plot: pygal.Graph):
    svg_unicode = plot.render(is_unicode=True)
    svg_unicode = make_series_unique(svg_unicode)
    svg_tree: xml.etree.ElementTree.Element = (
        xml.etree.ElementTree.fromstring(svg_unicode))
    svg_id = svg_tree.get("id")
    title: str = plot.config.title
    filename = title.lower().replace(" ", "_") + ".svg"
    return f"""
        <figure>
            {svg_unicode}
            <figcaption>
                <script>
                document.write(svgToDownloadLink("{svg_id}", "{filename}"));
                </script>
            </figcaption>
        </figure>
    """


def equidistant_ranges(length: int, parts: int) -> Iterator[Tuple[int, int]]:
    size = length // parts
    remainder = length % parts
    small_parts = parts - remainder
    start = 0
    for i in range(parts):
        part_size = size if i < small_parts else size + 1
        if part_size == 0:
            continue
        stop = start + part_size
        yield start, stop
        start = stop


def logarithmic_ranges(length: int, min_distance: int = 5):
    """
    Gives a squashed logarithmic range. It is not truly logarithmic as the
    minimum distance ensures that the lower units are more tightly packed.
    """
    # Use a scaling factor: this needs 400 units to reach the length of the
    # largest human chromosome. This will still fit on a graph once we reach
    # those sequencing sizes.
    scaling_factor = 250_000_000 ** (1 / 400)
    i = 0
    start = 0
    while True:
        stop = round(scaling_factor ** i)
        i += 1
        if stop >= start + min_distance:
            yield start, stop
            start = stop
            if stop >= length:
                return


def stringify_ranges(data_ranges: Iterable[Tuple[int, int]]):
    return [
        f"{start + 1}-{stop}" if start + 1 != stop else f"{start + 1}"
        for start, stop in data_ranges
    ]


def table_iterator(count_tables: array.ArrayType,
                   table_size: int) -> Iterator[memoryview]:
    table_view = memoryview(count_tables)
    for i in range(0, len(count_tables), table_size):
        yield table_view[i: i + table_size]


def aggregate_count_matrix(
        count_tables: array.ArrayType,
        data_ranges: Sequence[Tuple[int, int]],
        table_size: int) -> array.ArrayType:
    count_view = memoryview(count_tables)
    aggregated_matrix = array.array(
        "Q", bytes(8 * table_size * len(data_ranges)))
    ag_view = memoryview(aggregated_matrix)
    for cat_index, (start, stop) in enumerate(data_ranges):
        cat_offset = cat_index * table_size
        cat_view = ag_view[cat_offset:cat_offset + table_size]
        table_start = start * table_size
        table_stop = stop * table_size
        for i in range(table_size):
            cat_view[i] = sum(count_view[table_start + i: table_stop: table_size])
    return aggregated_matrix


def aggregate_base_tables(
        count_tables: array.ArrayType,
        data_ranges: Sequence[Tuple[int, int]],) -> array.ArrayType:
    return aggregate_count_matrix(count_tables, data_ranges, NUMBER_OF_NUCS)


def aggregate_phred_tables(
        count_tables: array.ArrayType,
        data_ranges: Sequence[Tuple[int, int]],) -> array.ArrayType:
    return aggregate_count_matrix(count_tables, data_ranges, NUMBER_OF_PHREDS)


def label_settings(x_labels: Sequence[str]) -> Dict[str, Any]:
    # Labels are ranges such as 1-5, 101-142 etc. This clutters the x axis
    # labeling so only use the first number. The values will be labelled
    # separately.
    simple_x_labels = [label.split("-")[0] for label in x_labels]
    if simple_x_labels and len(simple_x_labels[-1]) > 4:
        rotation = 45
    else:
        rotation = 0
    return dict(
        x_labels=simple_x_labels,
        x_labels_major_every=round(len(x_labels) / 30),
        x_label_rotation=rotation,
        show_minor_x_labels=False
    )


def label_values(values: Sequence[Any], labels: Sequence[Any]):
    if len(values) != len(labels):
        raise ValueError("labels and values should have the same length")
    return [{"value": value, "label": label} for value, label
            in zip(values, labels)]


@dataclasses.dataclass
class ReportModule(ABC):
    def __init__(self, *args, **kwargs):
        pass

    def from_dict(cls, d: Dict[str, Any]):
        return cls(**d)  # type: ignore

    def to_dict(self) -> Dict[str, Any]:
        return dataclasses.asdict(self)

    @abstractmethod
    def to_html(self) -> str:
        pass


@dataclasses.dataclass
class Meta(ReportModule):
    sequali_version: str
    report_generated: str
    filename: str
    filesize: int
    filename_read2: Optional[str]
    filesize_read2: Optional[int]

    @classmethod
    def from_filepath(cls, filepath: str, filepath_read2: Optional[str] = None):
        filename = os.path.basename(filepath)
        try:
            filesize = os.path.getsize(filepath)
        except OSError:
            filesize = 0
        if filepath_read2:
            filename_read2 = os.path.basename(filepath_read2)
            try:
                filesize_read2 = os.path.getsize(filepath_read2)
            except OSError:
                filesize_read2 = 0
        else:
            filename_read2 = None
            filesize_read2 = None
        timestamp = time.time()
        time_struct = time.localtime(timestamp)
        report_generated = time.strftime("%Y-%m-%d %H:%M:%S%z", time_struct)
        return cls(__version__, report_generated, filename, filesize,
                   filename_read2, filesize_read2)

    def to_html(self) -> str:
        content = io.StringIO()
        content.write(html_header("Metadata", 1))
        content.write(f"""
            <table>
            <tr><td>Filename</td><td><code>{self.filename}</code></td></tr>
            <tr><td>Filesize</td><td>{self.filesize / (1024 ** 3):.2f} GiB</td></tr>
        """)
        if self.filename_read2 is not None and self.filesize_read2 is not None:
            content.write(f"""
                <tr>
                    <td>Filename read 2</td>
                    <td><code>{self.filename_read2}</code></td>
                </tr>
                <tr>
                    <td>Filesize read 2</td>
                    <td>{self.filesize_read2 / (1024 ** 3):.2f} GiB</td>
                </tr>
            """)
        content.write(f"""
            <tr><td>Sequali version</td><td>{self.sequali_version}</td></tr>
            <tr><td>Report generated on</td><td>{self.report_generated}</td></tr>
            </table>
        """)
        return content.getvalue()


@dataclasses.dataclass
class Summary(ReportModule):
    mean_length: float
    minimum_length: int
    maximum_length: int
    total_reads: int
    q20_reads: int
    total_bases: int
    q20_bases: int
    total_gc_bases: int
    total_n_bases: int
    read_pair_info: Optional[str] = None

    def to_html(self) -> str:
        return f"""
            {html_header("Summary", 1, self.read_pair_info)}
            <table>
            <tr><td>Mean length</td><td style="text-align:right;">
                {self.mean_length:,.2f}</td><td></td></tr>
            <tr><td>Length range (min-max)</td><td style="text-align:right;">
                {self.minimum_length:,}</td>
                <td style="text-align:right;">{self.maximum_length:,}</td></tr>
            <tr>
                <td>Total reads</td>
                <td style="text-align:right;">{self.total_reads:,}</td>
                <td></td></tr>
            <tr>
                <td> Q20 reads</td>
                <td style="text-align:right;">{self.q20_reads:,}</td>
                <td style="text-align:right;">
                    {self.q20_reads / max(self.total_reads, 1):.2%}
                </td>
            </tr>
            <tr><td>Total bases</td><td style="text-align:right;">
                {self.total_bases:,}</td><td></td></tr>
            <tr>
                <td>Total GC bases</td>
                <td style="text-align:right;">
                    {self.total_gc_bases:,}
                </td>
                <td style="text-align:right;">
                    {self.total_gc_bases / max(
                        self.total_bases - self.total_n_bases, 1):.2%}
                </td>
            </tr>
            <tr>
                <td>Q20 bases</td>
                <td style="text-align:right;">
                    {self.q20_bases:,}
                </td>
                <td style="text-align:right;">
                    {self.q20_bases / max(self.total_bases, 1):.2%}
                </td>
            </tr>
            </table>
        """


@dataclasses.dataclass
class SequenceLengthDistribution(ReportModule):
    length_ranges: List[str]
    counts: List[int]
    q1: int
    q5: int
    q10: int
    q25: int
    q50: int
    q75: int
    q90: int
    q95: int
    q99: int
    read_pair_info: Optional[str] = None

    def plot(self) -> pygal.Graph:
        plot = pygal.Bar(
            title="Sequence length distribution",
            x_title="sequence length",
            y_title="number of reads",
            style=ONE_SERIE_STYLE,
            **label_settings(self.length_ranges),
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("Length", label_values(self.counts, self.length_ranges))
        return plot

    def distribution_table(self):
        if len({self.q1, self.q5, self.q10, self.q25, self.q50,
                self.q75, self.q90, self.q95, self.q99}) == 1:
            return ""
        return f"""
            <table>
                <tr><td>N1</td><td style="text-align:right;">{self.q1:,}</td></tr>
                <tr><td>N5</td><td style="text-align:right;">{self.q5:,}</td></tr>
                <tr><td>N10</td><td style="text-align:right;">{self.q10:,}</td></tr>
                <tr><td>N25</td><td style="text-align:right;">{self.q25:,}</td></tr>
                <tr><td>N50</td><td style="text-align:right;">{self.q50:,}</td></tr>
                <tr><td>N75</td><td style="text-align:right;">{self.q75:,}</td></tr>
                <tr><td>N90</td><td style="text-align:right;">{self.q90:,}</td></tr>
                <tr><td>N95</td><td style="text-align:right;">{self.q95:,}</td></tr>
                <tr><td>N99</td><td style="text-align:right;">{self.q99:,}</td></tr>
            </table>
        """

    def to_html(self):
        return f"""
            {html_header("Sequence length distribution", 1,
                         self.read_pair_info)}
            {self.distribution_table()}
            {figurize_plot(self.plot())}
        """

    @classmethod
    def from_base_count_tables(cls,
                               base_count_tables: array.ArrayType,
                               total_sequences: int,
                               data_ranges: Sequence[Tuple[int, int]],
                               read_pair_info: Optional[str] = None):
        max_length = len(base_count_tables) // NUMBER_OF_NUCS
        # use bytes constructor to initialize to 0
        sequence_lengths = array.array("Q", bytes(8 * (max_length + 1)))
        base_counts = array.array("Q", bytes(8 * (max_length + 1)))
        base_counts[0] = total_sequences  # all reads have at least 0 bases
        for i, table in enumerate(table_iterator(base_count_tables, NUMBER_OF_NUCS)):
            base_counts[i + 1] = sum(table)
        previous_count = 0
        for i in range(max_length, 0, -1):
            number_at_least = base_counts[i]
            sequence_lengths[i] = number_at_least - previous_count
            previous_count = number_at_least
        seqlength_view = memoryview(sequence_lengths)[1:]
        lengths = [sum(seqlength_view[start:stop]) for start, stop in
                   data_ranges]
        x_labels = stringify_ranges(data_ranges)
        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
        percentile_thresholds = [int(p * total_sequences / 100) for p in percentiles]
        thresh_iter = enumerate(percentile_thresholds)
        thresh_index, current_threshold = next(thresh_iter)
        accumulated_count = 0
        percentile_lengths = [0 for _ in percentiles]
        done = False
        for length, count in enumerate(sequence_lengths):
            while count > 0 and not done:
                remaining_threshold = current_threshold - accumulated_count
                if count > remaining_threshold:
                    accumulated_count += remaining_threshold
                    percentile_lengths[thresh_index] = length
                    count -= remaining_threshold
                    try:
                        thresh_index, current_threshold = next(thresh_iter)
                    except StopIteration:
                        done = True
                        break
                    continue
                break
            accumulated_count += count
            if done:
                break

        return cls(["0"] + x_labels, [sequence_lengths[0]] + lengths,
                   *percentile_lengths, read_pair_info=read_pair_info)  # type: ignore


@dataclasses.dataclass
class PerPositionMeanQualityAndSpread(ReportModule):
    x_labels: List[str]
    percentiles: List[Tuple[str, List[float]]]
    read_pair_info: Optional[str] = None

    def plot(self) -> pygal.Graph:
        plot = pygal.Line(
            title="Per position quality percentiles",
            show_dots=False,
            x_title="position",
            y_title="phred score",
            y_labels=list(range(0, 51, 10)),
            range=(0.0, 50.0),
            style=pygal.style.DefaultStyle(
                colors=["#000000"] * 12,
                **COMMON_GRAPH_STYLE_OPTIONS,
            ),
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        percentiles = dict(self.percentiles)
        plot.add("top 1%", label_values(percentiles["top 1%"], self.x_labels),
                 stroke_style={"dasharray": '1,2'})
        plot.add("top 5%", label_values(percentiles["top 5%"], self.x_labels),
                 stroke_style={"dasharray": '3,3'})
        plot.add("mean", label_values(percentiles["mean"], self.x_labels),
                 show_dots=True, dots_size=1)
        plot.add("bottom 5%", label_values(percentiles["bottom 5%"], self.x_labels),
                 stroke_style={"dasharray": '3,3'})
        plot.add("bottom 1%", label_values(percentiles["bottom 1%"], self.x_labels),
                 stroke_style={"dasharray": '1,2'})
        return plot

    def to_html(self):
        return f"""
            {html_header("Per position quality percentiles", 1,
                         self.read_pair_info)}
            <p class="explanation">Shows the mean for all bases and the means
            of the lowest and
            highest percentiles to indicate the spread. Since the graph is
            based on the sampled categories, rather than exact phreds, it is
            an approximation.</p>
            {figurize_plot(self.plot())}
        """

    @classmethod
    def from_phred_table_and_labels(cls,
                                    phred_tables: array.ArrayType,
                                    x_labels,
                                    read_pair_info: Optional[str] = None):
        percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]
        percentile_fractions = [i / 100 for i in percentiles]
        total_tables = len(phred_tables) // NUMBER_OF_PHREDS
        percentile_tables = [[0.0 for _ in range(total_tables)]
                             for _ in percentiles]
        reversed_percentile_tables = [[0.0 for _ in range(total_tables)]
                                      for _ in percentiles]
        mean = [0.0 for _ in range(total_tables)]
        for cat_index, table in enumerate(
                table_iterator(phred_tables, NUMBER_OF_PHREDS)):
            total = sum(table)
            if total == 0:
                continue
            total_error_rate = sum(
                PHRED_INDEX_TO_ERROR_RATE[i] * x for i, x in enumerate(table))
            percentile_thresholds = [int(f * total) for f in percentile_fractions]
            mean[cat_index] = -10 * math.log10(total_error_rate / total)
            accumulated_count = 0
            accumulated_errors = 0.0
            threshold_iter = enumerate(percentile_thresholds)
            thresh_index, current_threshold = next(threshold_iter)
            for phred_index, count in enumerate(table):
                while count > 0:
                    remaining_threshold = current_threshold - accumulated_count
                    if count > remaining_threshold:
                        accumulated_errors += (remaining_threshold *
                                               PHRED_INDEX_TO_ERROR_RATE[phred_index])
                        accumulated_count += remaining_threshold
                        if accumulated_count > 0:
                            percentile_tables[thresh_index][cat_index] = (
                                -10 * math.log10(
                                    accumulated_errors / accumulated_count))
                            reversed_percentile_tables[thresh_index][cat_index] = (
                                -10 * math.log10(
                                    (total_error_rate - accumulated_errors) /
                                    (total - accumulated_count)
                                )
                            )
                        count -= remaining_threshold
                        try:
                            thresh_index, current_threshold = next(threshold_iter)
                        except StopIteration:
                            # This will make sure the next cat_index is reached
                            # since 2 ** 65 will not be reached
                            thresh_index = sys.maxsize
                            current_threshold = 2**65
                        continue
                    break
                accumulated_count += count
                accumulated_errors += PHRED_INDEX_TO_ERROR_RATE[phred_index] * count
        graph_series = [
            ("bottom 1%", percentile_tables[0]),
            ("bottom 5%", percentile_tables[1]),
            ("bottom 10%", percentile_tables[2]),
            ("bottom 25%", percentile_tables[3]),
            ("bottom 50%", percentile_tables[4]),
            ("mean", mean),
            ("top 50%", reversed_percentile_tables[-5]),
            ("top 25%", reversed_percentile_tables[-4]),
            ("top 10%", reversed_percentile_tables[-3]),
            ("top 5%", reversed_percentile_tables[-2]),
            ("top 1%", reversed_percentile_tables[-1]),
        ]
        return cls(
            x_labels=x_labels,
            percentiles=graph_series,
            read_pair_info=read_pair_info,
            )


@dataclasses.dataclass
class PerBaseQualityScoreDistribution(ReportModule):
    x_labels: Sequence[str]
    series: Sequence[Sequence[float]]
    read_pair_info: Optional[str] = None

    @classmethod
    def from_phred_count_table_and_labels(
            cls, phred_tables: array.ArrayType, x_labels: Sequence[str],
            read_pair_info: Optional[str] = None
    ):
        total_tables = len(x_labels)
        quality_distribution = [
            [0.0 for _ in range(total_tables)]
            for _ in range(NUMBER_OF_PHREDS)
        ]
        for cat_index, table in enumerate(
                table_iterator(phred_tables, NUMBER_OF_PHREDS)):
            total_nucs = sum(table)
            if total_nucs == 0:
                continue
            for offset, phred_count in enumerate(table):
                if phred_count == 0:
                    continue
                nuc_fraction = phred_count / total_nucs
                quality_distribution[offset][cat_index] = nuc_fraction
        return cls(x_labels, quality_distribution, read_pair_info=read_pair_info)

    def plot(self) -> pygal.Graph:
        plot = pygal.StackedBar(
            title="Per base quality distribution",
            style=QUALITY_DISTRIBUTION_STYLE,
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="position",
            y_title="fraction",
            fill=True,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        for name, serie in zip(QUALITY_SERIES_NAMES, self.series):
            serie_filled = sum(serie) > 0.0
            plot.add(name, label_values(serie, self.x_labels),
                     show_dots=serie_filled)
        return plot

    def to_html(self):
        return f"""
            {html_header("Per position quality score distribution",
                         1, self.read_pair_info)}
            {figurize_plot(self.plot())}
        """


@dataclasses.dataclass
class PerSequenceAverageQualityScores(ReportModule):
    average_quality_counts: Sequence[int]
    x_labels: Tuple[str, ...] = tuple(str(x) for x in range(PHRED_MAX + 1))
    read_pair_info: Optional[str] = None

    def plot(self) -> pygal.Graph:
        maximum_score = 0
        for i, count in enumerate(self.average_quality_counts):
            if count > 0:
                maximum_score = i
        maximum_score = max(maximum_score + 2, 40)
        plot = pygal.Bar(
            title="Per sequence quality scores",
            x_labels=range(maximum_score + 1),
            x_labels_major_every=3,
            show_minor_x_labels=False,
            style=ONE_SERIE_STYLE,
            x_title="Phred score",
            y_title="Percentage of total",
            **COMMON_GRAPH_OPTIONS
        )
        total = sum(self.average_quality_counts)
        if total == 0:
            percentage_scores = [0.0 for _ in self.average_quality_counts]
        else:
            percentage_scores = [100 * score / total
                                 for score in self.average_quality_counts]

        plot.add("", percentage_scores[:maximum_score])
        return plot

    def to_html(self) -> str:
        return f"""
            {html_header("Per sequence average quality scores", 1,
                         self.read_pair_info)}
            {self.quality_scores_table()}
            {figurize_plot(self.plot())}
        """

    def quality_scores_table(self) -> str:
        table = io.StringIO()
        total = max(sum(self.average_quality_counts), 1)
        fractions = [count / total for count in self.average_quality_counts]
        table.write("<table>")
        for i in (5, 7, 10, 12, 15, 20, 30):
            table.write(
                f"""
                <tr>
                    <td>{html.escape(">=Q")}{i}</td>
                    <td style="text-align:right;">
                        {sum(self.average_quality_counts[i:]):,}
                    </td>
                    <td style="text-align:right;">
                        {sum(fractions[i:]):.2%}
                    </td>
                </tr>
                """
            )
        table.write("</table>")
        return table.getvalue()

    @classmethod
    def from_qc_metrics(cls, metrics: QCMetrics,
                        read_pair_info: Optional[str] = None):
        return cls(list(metrics.phred_scores()), read_pair_info=read_pair_info)


@dataclasses.dataclass
class PerPositionBaseContent(ReportModule):
    x_labels: Sequence[str]
    A: Sequence[float]
    C: Sequence[float]
    G: Sequence[float]
    T: Sequence[float]
    read_pair_info: Optional[str] = None

    def plot(self) -> pygal.Graph:
        style_class = pygal.style.Style
        green = COLOR_GREEN
        dark_green = "#228B22"  # ForestGreen
        blue = "#00BFFF"  # DeepSkyBlue
        dark_blue = "#1E90FF"  # DodgerBlue
        black = "#000000"
        style = style_class(
            colors=(green, dark_green, blue, dark_blue, black),
            **COMMON_GRAPH_STYLE_OPTIONS
        )
        plot = pygal.StackedLine(
            title="Base content",
            style=style,
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="position",
            y_title="fraction",
            fill=True,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("G", label_values(self.G, self.x_labels))
        plot.add("C", label_values(self.C, self.x_labels))
        plot.add("A", label_values(self.A, self.x_labels))
        plot.add("T", label_values(self.T, self.x_labels))
        return plot

    def to_html(self) -> str:
        return f"""
             {html_header("Per position base content", 1,
                          self.read_pair_info)}
             {figurize_plot(self.plot())}
        """

    @classmethod
    def from_base_count_tables_and_labels(cls,
                                          base_count_tables: array.ArrayType,
                                          labels: Sequence[str],
                                          read_pair_info: Optional[str] = None,
                                          ):
        total_tables = len(base_count_tables) // NUMBER_OF_NUCS
        base_fractions = [
            [0.0 for _ in range(total_tables)]
            for _ in range(NUMBER_OF_NUCS)
        ]
        for index, table in enumerate(
                table_iterator(base_count_tables, NUMBER_OF_NUCS)):
            total_bases = sum(table)
            n_bases = table[N]
            named_total = total_bases - n_bases
            if named_total == 0:
                continue
            base_fractions[A][index] = table[A] / named_total
            base_fractions[C][index] = table[C] / named_total
            base_fractions[G][index] = table[G] / named_total
            base_fractions[T][index] = table[T] / named_total
        return cls(
            labels,
            A=base_fractions[A],
            C=base_fractions[C],
            G=base_fractions[G],
            T=base_fractions[T],
            read_pair_info=read_pair_info,
        )


@dataclasses.dataclass
class PerPositionNContent(ReportModule):
    x_labels: Sequence[str]
    n_content: Sequence[float]
    read_pair_info: Optional[str] = None

    @classmethod
    def from_base_count_tables_and_labels(
            cls, base_count_tables: array.ArrayType, labels: Sequence[str],
            read_pair_info: Optional[str] = None,
    ):
        total_tables = len(base_count_tables) // NUMBER_OF_NUCS
        n_fractions = [0.0 for _ in range(total_tables)]
        for index, table in enumerate(
                table_iterator(base_count_tables, NUMBER_OF_NUCS)):
            total_bases = sum(table)
            if total_bases == 0:
                continue
            n_fractions[index] = table[N] / total_bases
        return cls(
            labels,
            n_fractions,
            read_pair_info,
        )

    def plot(self) -> pygal.Graph:
        plot = pygal.Bar(
            title="Per position N content",
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="position",
            y_title="fraction",
            fill=True,
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("N", label_values(self.n_content, self.x_labels))
        return plot

    def to_html(self) -> str:
        return f"""
            {html_header("Per position N content", 1,
                         self.read_pair_info)}
            {figurize_plot(self.plot())}
        """


@dataclasses.dataclass
class PerSequenceGCContent(ReportModule):
    gc_content_counts: Sequence[int]
    smoothened_gc_content_counts: Sequence[int]
    x_labels: Sequence[str] = tuple(str(x) for x in range(101))
    smoothened_x_labels: Sequence[str] = tuple(str(x) for x in range(0, 101, 2))
    read_pair_info: Optional[str] = None

    def plot(self) -> pygal.Graph:
        plot = pygal.Bar(
            title="Per sequence GC content",
            x_labels=self.x_labels,
            x_labels_major_every=3,
            show_minor_x_labels=False,
            x_title="GC %",
            y_title="number of reads",
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("", self.gc_content_counts)
        return plot

    def smoothened_plot(self):
        plot = pygal.Line(
            title="Per sequence GC content (smoothened)",
            x_labels=self.smoothened_x_labels,
            x_labels_major_every=3,
            show_minor_x_labels=False,
            x_title="GC %",
            y_title="number of reads",
            interpolate="cubic",
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS,
        )
        plot.add("", self.smoothened_gc_content_counts)
        return plot

    def to_html(self) -> str:
        return f"""
            {html_header("Per sequence GC content", 1,
                         self.read_pair_info)}
            <p class="explanation">
            For short reads with fixed size (i.e. Illumina) the plot will
            look very spiky due to the GC content calculation: GC bases / all
            bases. For read lengths of 151, both 75 and 76 GC bases lead to a
            percentage of 50% (rounded) and 72 and 73 GC bases leads to 48%
            (rounded). Only 74 GC bases leads to 49%. As a result the
            even categories will be twice as high, which creates a spike. The
            smoothened plot is provided to give a clearer picture in this case.
            </p>
            {figurize_plot(self.plot())}
            {figurize_plot(self.smoothened_plot())}
        """

    @classmethod
    def from_qc_metrics(cls, metrics: QCMetrics, read_pair_info: Optional[str] = None):
        gc_content = list(metrics.gc_content())
        smoothened_gc_content = []
        gc_content_iter = iter(gc_content)
        for i in range(50):
            smoothened_gc_content.append(next(gc_content_iter) + next(gc_content_iter))
        # Append the last 100% category.
        smoothened_gc_content.append(next(gc_content_iter))
        return cls(gc_content, smoothened_gc_content, read_pair_info=read_pair_info)


@dataclasses.dataclass
class AdapterContent(ReportModule):
    x_labels: Sequence[str]
    adapter_content: Sequence[Tuple[str, Sequence[float]]]
    read_pair_info: Optional[str] = None

    def plot(self) -> pygal.Graph:
        plot = pygal.Line(
            title="Adapter content (%)",
            range=(0.0, 100.0),
            x_title="position",
            y_title="%",
            legend_at_bottom=True,
            legend_at_bottom_columns=1,
            truncate_legend=-1,
            style=MULTIPLE_SERIES_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        adapter_content = [(label, content) for label, content in
                           self.adapter_content if content and max(content) >= 0.1]
        adapter_content.sort(key=lambda x: max(x[1]),
                             reverse=True)
        for label, content in adapter_content:
            plot.add(label, label_values(content, self.x_labels))
        return plot

    def to_html(self):
        return f"""
            {html_header("Adapter content", 1, self.read_pair_info)}
            <p class="explanation">Only adapters that are present more than 0.1%
            are shown. Given the 12&#8239;bp
            length of the sequences used to estimate the content, values below this
            threshold are problably false positives. The legend is sorted from
            most frequent to least frequent.</p>
            <p class="explanation">For nanopore the the adapter mix (AMX) and
            ligation kit have overlapping adapter sequences and are therefore
            indistinguishable. Please consult the
            <a href="https://help.nanoporetech.com/en/articles/6632917-what-are-the-adapter-sequences-used-in-the-kits">
            nanopore documentation</a> for more information which adapters are
            used by your kit.</p>
            <p class="explanation">For illumina short reads, the last part of
            the graph will be flat as the 12&#8239;bp probes cannot be found in
            the last 11 base pairs.</p>
            {figurize_plot(self.plot())}
        """  # noqa: E501

    @classmethod
    def from_adapter_counter_adapters_and_ranges(
            cls, adapter_counter: AdapterCounter, adapters: Sequence[Adapter],
            data_ranges: Sequence[Tuple[int, int]],
            read_pair_info: Optional[str] = None,
    ):

        def accumulate_counts(counts: Iterable[int]) -> List[int]:
            total = 0
            accumulated_counts = []
            for count in counts:
                total += count
                accumulated_counts.append(total)
            return accumulated_counts

        all_adapters = []
        sequence_to_adapter = {adapter.sequence: adapter for adapter in adapters}
        adapter_names = [adapter.name for adapter in adapters]
        total_sequences = adapter_counter.number_of_sequences
        for adapter_sequence, countarray in adapter_counter.get_counts():
            adapter = sequence_to_adapter[adapter_sequence]
            adapter_counts = [sum(countarray[start:stop])
                              for start, stop in data_ranges]
            if adapter.sequence_position == "end":
                accumulated_counts = accumulate_counts(adapter_counts)
            else:
                # Reverse the counts, accumulate them and reverse again for
                # adapters at the front.
                accumulated_counts = list(reversed(
                    accumulate_counts(reversed(adapter_counts))))
            all_adapters.append([count * 100 / total_sequences
                                 for count in accumulated_counts])
        return cls(stringify_ranges(data_ranges),
                   list(zip(adapter_names, all_adapters)),
                   read_pair_info=read_pair_info)


@dataclasses.dataclass
class PerTileQualityReport(ReportModule):
    x_labels: Sequence[str]
    normalized_per_tile_averages: Sequence[Tuple[str, Sequence[float]]]
    tiles_2x_errors: Sequence[str]
    tiles_10x_errors: Sequence[str]
    skipped_reason: Optional[str]
    read_pair_info: Optional[str] = None

    @classmethod
    def from_per_tile_quality_and_ranges(
            cls, ptq: PerTileQuality, data_ranges: Sequence[Tuple[int, int]],
            read_pair_info: Optional[str] = None,
    ):
        if ptq.skipped_reason:
            return cls([], [], [], [], ptq.skipped_reason)
        average_phreds = []
        per_category_totals = [0.0 for i in range(len(data_ranges))]
        tile_counts = ptq.get_tile_counts()
        for tile, summed_errors, counts in tile_counts:
            range_averages = [
                sum(summed_errors[start:stop]) / max(sum(counts[start:stop]), 1)
                for start, stop in data_ranges]
            range_phreds = []
            for i, average in enumerate(range_averages):
                if average != 0:
                    phred = -10 * math.log10(average)
                else:
                    phred = 0
                range_phreds.append(phred)
                # Averaging phreds takes geometric mean.
                per_category_totals[i] += phred
            average_phreds.append((tile, range_phreds))
        number_of_tiles = len(tile_counts)
        averages_per_category = [total / number_of_tiles
                                 for total in per_category_totals]
        normalized_averages = []
        tiles_2x_errors = []
        tiles_10x_errors = []
        for tile, tile_phreds in average_phreds:
            if not tile_phreds:
                continue
            normalized_tile_phreds = [
                tile_phred - average
                for tile_phred, average in
                zip(tile_phreds, averages_per_category)
            ]
            lowest_phred = min(normalized_tile_phreds)
            if lowest_phred <= -10.0:
                tiles_10x_errors.append(str(tile))
            elif lowest_phred <= -3.0:
                tiles_2x_errors.append(str(tile))
            normalized_averages.append((str(tile), normalized_tile_phreds))
        return cls(
            x_labels=stringify_ranges(data_ranges),
            normalized_per_tile_averages=normalized_averages,
            tiles_2x_errors=tiles_2x_errors,
            tiles_10x_errors=tiles_10x_errors,
            skipped_reason=ptq.skipped_reason,
            read_pair_info=read_pair_info,
        )

    def plot(self) -> pygal.Graph:
        style_colors = MULTIPLE_SERIES_STYLE.colors
        red = "#FF0000"
        yellow = "#FFD700"  # actually 'Gold' which is darker and more visible.
        style = pygal.style.Style(
            colors=(yellow, red) + style_colors,
            **COMMON_GRAPH_STYLE_OPTIONS,
        )
        scatter_plot = pygal.Line(
            title="Deviation from geometric mean in phred units.",
            x_title="position",
            stroke=False,
            style=style,
            y_title="Normalized phred",
            truncate_legend=-1,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )

        def add_horizontal_line(name, position):
            scatter_plot.add(name,
                             [position for _ in range(len(self.x_labels))],
                             show_dots=False, stroke=True, )

        add_horizontal_line("2 times more errors", -3)
        add_horizontal_line("10 times more errors", -10)

        min_phred = -10.0
        max_phred = 0.0
        for tile, tile_phreds in self.normalized_per_tile_averages:
            min_phred = min(min_phred, *tile_phreds)
            max_phred = max(max_phred, *tile_phreds)
            scatter_plot.range = (min_phred - 1, max_phred + 1)
            if min(tile_phreds) > -3 and max(tile_phreds) < 3:
                continue
            cleaned_phreds = [{'value': phred, 'label': label}
                              if (phred > 3 or phred < -3) else None
                              for phred, label in
                              zip(tile_phreds, self.x_labels)]
            scatter_plot.add(str(tile), cleaned_phreds)

        return scatter_plot

    def to_html(self) -> str:
        header = html_header("Per tile quality", 1, self.read_pair_info)
        if self.skipped_reason:
            return header + (f"Per tile quality skipped. Reason: "
                             f"{self.skipped_reason}.")
        return header + f"""
            <p class="explanation">
            This graph shows the deviation of each tile on each position from
            the geometric mean of all tiles at that position. The scale is
            expressed in phred units. -10 is 10 times more errors than the
            average.
            -3 is ~2 times more errors than the average. Only points that
            deviate more than 2 phred units from the average are shown. </p>
            <p>Tiles with more than 2 times the average error:
                {", ".join(self.tiles_2x_errors)}</p>
            <p>Tiles with more than 10 times the average error:
                {", ".join(self.tiles_10x_errors)}</p>
            {figurize_plot(self.plot())}
        """


@dataclasses.dataclass
class DuplicationCounts(ReportModule):
    tracked_unique_sequences: int
    duplication_counts: Sequence[Tuple[int, int]]
    remaining_fraction: float
    estimated_duplication_fractions: Dict[str, float]
    fingerprint_front_sequence_length: int
    fingerprint_back_sequence_length: int
    fingerprint_front_sequence_offset: int
    fingerprint_back_sequence_offset: int

    def plot(self) -> pygal.Graph:
        plot = pygal.Bar(
            title="Duplication levels (%)",
            x_labels=list(self.estimated_duplication_fractions.keys()),
            x_title="Duplication counts",
            y_title="Percentage of total",
            x_label_rotation=45,
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("",
                 [100 * fraction for fraction in
                  self.estimated_duplication_fractions.values()])
        return plot

    def to_html(self):
        first_part = f"""
        <p class="explanation">
        Fingerprints are taken by taking a sample from the beginning and the
        end at an offset. The samples are combined and hashed while using the
        length as a seed. A subsample of these fingerprints is stored to
        estimate the duplication rate. See
        <a href="https://sequali.readthedocs.io/#duplication-estimation-module">
        the documentation</a> for a complete explanation.</p>
        <table>
            <tr>
                <td>Fingerprint front sequence length</td>
                <td style="text-align:right;">
                    {self.fingerprint_front_sequence_length:,}
                </td>
            </tr>
            <tr>
                <td>Fingerprint front sequence offset</td>
                <td style="text-align:right;">
                    {self.fingerprint_front_sequence_offset:,}
                </td>
            </tr>
            <tr>
                <td>Fingerprint back sequence length</td>
                <td style="text-align:right;">
                    {self.fingerprint_back_sequence_length}
                </td>
            </tr>
            <tr>
                <td>Fingerprint back sequence offset</td>
                <td style="text-align:right;">
                    {self.fingerprint_back_sequence_offset:,}
                </td>
            </tr>
            <tr>
                <td>Subsampled fingerprints</td>
                <td style="text-align:right;">
                    {self.tracked_unique_sequences:,}
                </td>
            </tr>
            <tr>
                <td>Estimated remaining sequences if deduplicated</td>
                <td style="text-align:right;">{self.remaining_fraction:.2%}</td>
            </tr>
            </table>
        """
        return f"""
            {html_header("Duplication percentages", 1)}
            {first_part}
            {figurize_plot(self.plot())}
        """

    @staticmethod
    def estimated_counts_to_fractions(
            estimated_counts: Iterable[Tuple[int, int]]):
        named_slices = {
            "1": slice(1, 2),
            "2": slice(2, 3),
            "3": slice(3, 4),
            "4": slice(4, 5),
            "5": slice(5, 6),
            "6-10": slice(6, 11),
            "11-20": slice(11, 21),
            "21-30": slice(21, 31),
            "31-50": slice(31, 51),
            "51-100": slice(51, 101),
            "101-500": slice(101, 501),
            "501-1000": slice(501, 1001),
            "1001-5000": slice(1001, 5001),
            "5001-10000": slice(5001, 10_001),
            "10001-50000": slice(10_001, 50_001),
            "> 50000": slice(50_001, None),
        }
        count_array = array.array("Q", bytes(8 * 50002))
        for duplication, count in estimated_counts:
            if duplication > 50_000:
                count_array[50_001] += count * duplication
            else:
                count_array[duplication] = count * duplication
        total = max(sum(count_array), 1)
        aggregated_fractions = [
            sum(count_array[slc]) / total for slc in named_slices.values()
        ]
        return dict(zip(named_slices.keys(), aggregated_fractions))

    @staticmethod
    def deduplicated_fraction(duplication_counts: Dict[int, int]):
        total_sequences = sum(duplicates * count
                              for duplicates, count in
                              duplication_counts.items())
        unique_sequences = sum(duplication_counts.values())
        return unique_sequences / max(total_sequences, 1)

    @classmethod
    def from_dedup_estimator(cls, dedup_est: DedupEstimator):

        tracked_unique_sequences = dedup_est.tracked_sequences
        duplication_counts = dedup_est.duplication_counts()
        duplication_categories = collections.Counter(duplication_counts)
        estimated_duplication_fractions = cls.estimated_counts_to_fractions(
            duplication_categories.items())
        deduplicated_fraction = cls.deduplicated_fraction(
            duplication_categories)
        return cls(
            tracked_unique_sequences=tracked_unique_sequences,
            duplication_counts=sorted(duplication_categories.items()),
            estimated_duplication_fractions=estimated_duplication_fractions,
            remaining_fraction=deduplicated_fraction,
            fingerprint_front_sequence_length=dedup_est.front_sequence_length,
            fingerprint_back_sequence_length=dedup_est.back_sequence_length,
            fingerprint_front_sequence_offset=dedup_est.front_sequence_offset,
            fingerprint_back_sequence_offset=dedup_est.back_sequence_offset,
        )


class OverRepresentedSequence(typing.NamedTuple):
    count: int  # type: ignore
    fraction: float
    sequence: str
    revcomp_sequence: str
    most_matches: int
    max_matches: int
    best_match: str


@dataclasses.dataclass
class OverRepresentedSequences(ReportModule):
    overrepresented_sequences: List[OverRepresentedSequence]
    max_unique_fragments: int
    collected_fragments: int
    sample_every: int
    sequence_length: int
    total_fragments: int
    total_sequences: int
    sampled_sequences: int
    read_pair_info: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {"overrepresented_sequences":
                [x._asdict() for x in self.overrepresented_sequences],
                "max_unique_fragments": self.max_unique_fragments,
                "sample_every": self.sample_every,
                "collected_fragments": self.collected_fragments,
                "sequence_length": self.sequence_length,
                "total_fragments": self.total_fragments,
                "total_sequences": self.total_sequences,
                "sampled_sequences": self.sampled_sequences,
                "read_pair_info": self.read_pair_info}

    def from_dict(cls, d: Dict[str, List[Dict[str, Any]]]):
        overrepresented_sequences = d["overrepresented_sequences"]
        return cls([OverRepresentedSequence(**d)
                   for d in overrepresented_sequences],
                   max_unique_fragments=d["max_unique_fragments"],
                   collected_fragments=d["collected_fragments"],
                   sample_every=d["sample_every"],
                   sequence_length=d["sequence_length"],
                   total_fragments=d["total_fragments"],
                   total_sequences=d["total_sequences"],
                   sampled_sequences=d["sampled_sequences"],
                   read_pair_info=d["read_pair_info"])  # type: ignore

    def to_html(self) -> str:
        header = html_header("Overrepresented sequences", 1,
                             self.read_pair_info)
        if len(self.overrepresented_sequences) == 0:
            return header + "No overrepresented sequences."
        content = io.StringIO()
        content.write(header)
        content.write(
            f"""
            <p class="explanation">A subsample of the sequences is analysed
            Sequences are cut into fragments of up to 31&#8239;bp. Fragments
            are stored and counted. When the maximum amount of unique fragments
            is reached, only fragments that are already stored are counted. The
            rest of the fragments is ignored.
            Fragments are stored in their canonical representation. That is
            either the sequence or the reverse complement, whichever has
            the lowest sort order. Both representations are shown in the
            table. See
            <a href="https://sequali.readthedocs.io/#overrepresented-sequences-module">
            the documentation for a complete explanation.</a>
            </p>
            <p class="explanation">
                The percentage shown is an estimate based on the number of
                occurences of the fragment in relation to the number of
                sampled sequences. This makes the assumption that a
                fragment only occurs once in each sequence.
            </p>
            <table>
            <tr>
                <td>Total sequences in file</td>
                <td style="text-align:right;">{self.total_sequences:,}</td>
            </tr>
            <tr>
                <td>Sampled sequences</td>
                <td style="text-align:right;">{self.sampled_sequences:,}</td>
            </tr>
            <tr>
                <td>Sampling rate</td>
                <td style="text-align:right;">1 in {self.sample_every}</td>
            </tr>
            <tr>
                <td>Total fragments sampled</td>
                <td style="text-align:right;">{self.total_fragments:,}</td>
            </tr>
            <tr>
                <td>Stored unique fragments</td>
                <td style="text-align:right;">{self.collected_fragments:,}</td>
            </tr>
            <tr>
                <td>Maximum unique fragments</td>
                <td style="text-align:right;">{self.max_unique_fragments:,}</td>
            </tr>
            <tr>
                <td>Fragment size</td>
                <td style="text-align:right;">{self.sequence_length}</td>
            </tr>
            </table>
            """
        )
        content.write("<table>")
        content.write("<tr><th>count</th><th>percentage</th>"
                      "<th>canonical sequence</th>"
                      "<th>reverse complemented sequence</th>"
                      "<th>kmers (matched/max)</th>"
                      "<th>best match</th></tr>")
        for item in self.overrepresented_sequences:
            content.write(
                f"""<tr><td style="text-align:right;">{item.count}</td>
                    <td style="text-align:right;">{item.fraction:.2%}</td>
                    <td style="text-align:center;font-family:monospace;">
                        {item.sequence}</td>
                    <td style="text-align:center;font-family:monospace;">
                        {item.revcomp_sequence}</td>
                    <td>({item.most_matches}/{item.max_matches})</td>
                    <td>{html.escape(item.best_match)}</td></tr>""")
        content.write("</table>")
        return content.getvalue()

    @classmethod
    def from_sequence_duplication(
            cls,
            seqdup: SequenceDuplication,
            fraction_threshold: float = DEFAULT_FRACTION_THRESHOLD,
            min_threshold: int = DEFAULT_MIN_THRESHOLD,
            max_threshold: int = DEFAULT_MAX_THRESHOLD,
            read_pair_info: Optional[str] = None,
    ):
        overrepresented_sequences = seqdup.overrepresented_sequences(
            fraction_threshold,
            min_threshold,
            max_threshold
        )

        overrepresented_with_identification = [
            OverRepresentedSequence(
                count, fraction, sequence, reverse_complement(sequence),
                *identify_sequence_builtin(sequence))
            for count, fraction, sequence in overrepresented_sequences
        ]
        return cls(overrepresented_with_identification,
                   seqdup.max_unique_fragments,
                   seqdup.collected_unique_fragments,
                   seqdup.sample_every,
                   seqdup.fragment_length,
                   seqdup.total_fragments,
                   seqdup.number_of_sequences,
                   seqdup.sampled_sequences,
                   read_pair_info=read_pair_info)


@dataclasses.dataclass
class NanoStatsReport(ReportModule):
    x_labels: List[str]
    time_bases: List[int]
    time_reads: List[int]
    time_active_channels: List[int]
    qual_percentages_over_time: List[List[float]]
    per_channel_bases: Dict[int, int]
    per_channel_quality: Dict[int, float]
    translocation_speed: List[int]
    skipped_reason: Optional[str] = None

    @staticmethod
    def seconds_to_hour_minute_notation(seconds: int):
        minutes = seconds // 60
        hours = minutes // 60
        minutes %= 60
        return f"{hours:02}:{minutes:02}"

    @classmethod
    def from_nanostats(cls, nanostats: NanoStats):
        if nanostats.skipped_reason:
            return cls(
                [],
                [],
                [],
                [],
                [],
                {},
                {},
                [],
                nanostats.skipped_reason
            )
        run_start_time = nanostats.minimum_time
        run_end_time = nanostats.maximum_time
        duration = run_end_time - run_start_time
        time_slots = 200
        time_per_slot = duration / time_slots
        time_interval_minutes = (math.ceil(time_per_slot) + 59) // 60
        time_interval = max(time_interval_minutes * 60, 1)
        # Use duration + 1 to avoid cases where time_interval x time_slots == duration
        # which causes the last slot to be missing.
        time_ranges = [(start, start + time_interval)
                       for start in range(0, duration + 1, time_interval)]
        time_slots = len(time_ranges)
        time_active_slots_sets: List[Set[int]] = [set() for _ in
                                                  range(time_slots)]
        time_bases = [0 for _ in range(time_slots)]
        time_reads = [0 for _ in range(time_slots)]
        time_qualities = [[0 for _ in range(12)] for _ in
                          range(time_slots)]
        per_channel_bases: Dict[int, int] = defaultdict(lambda: 0)
        per_channel_cumulative_error: Dict[int, float] = defaultdict(lambda: 0.0)
        translocation_speeds = [0] * 81
        for readinfo in nanostats.nano_info_iterator():
            relative_start_time = readinfo.start_time - run_start_time
            timeslot = relative_start_time // time_interval
            length = readinfo.length
            cumulative_error_rate = readinfo.cumulative_error_rate
            channel_id = readinfo.channel_id
            if length:
                phred = round(
                    -10 * math.log10(cumulative_error_rate / length))
            else:
                phred = 0
            phred_index = min(phred, 47) >> 2
            time_active_slots_sets[timeslot].add(channel_id)
            time_bases[timeslot] += length
            time_reads[timeslot] += 1
            time_qualities[timeslot][phred_index] += 1
            per_channel_bases[channel_id] += length
            per_channel_cumulative_error[channel_id] += cumulative_error_rate
            read_duration = readinfo.duration
            if read_duration:
                translocation_speed = min(round(length / read_duration), 800)
                translocation_speed //= 10
                translocation_speeds[translocation_speed] += 1
        per_channel_quality: Dict[int, float] = {}
        for channel, error_rate in per_channel_cumulative_error.items():
            total_bases = per_channel_bases[channel]
            if total_bases:
                phred_score = -10 * math.log10(error_rate / total_bases)
            else:
                phred_score = 0
            per_channel_quality[channel] = phred_score
        qual_percentages_over_time: List[List[float]] = [[] for _ in
                                                         range(12)]
        for quals in time_qualities:
            total = sum(quals)
            for i, q in enumerate(quals):
                qual_percentages_over_time[i].append(q / max(total, 1))
        time_active_slots = [len(s) for s in time_active_slots_sets]
        return cls(
            x_labels=[f"{cls.seconds_to_hour_minute_notation(start)}-"
                      f"{cls.seconds_to_hour_minute_notation(stop)}"
                      for start, stop in time_ranges],
            qual_percentages_over_time=qual_percentages_over_time,
            time_active_channels=time_active_slots,
            time_bases=time_bases,
            time_reads=time_reads,
            per_channel_bases=dict(sorted(per_channel_bases.items())),
            per_channel_quality=dict(sorted(per_channel_quality.items())),
            translocation_speed=translocation_speeds,
            skipped_reason=nanostats.skipped_reason
        )

    def time_bases_plot(self):
        plot = pygal.Bar(
            title="Base count over time",
            x_title="time(HH:MM)",
            y_title="base count",
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", label_values(self.time_bases, self.x_labels))
        return plot

    def time_reads_plot(self):
        plot = pygal.Bar(
            title="Number of reads over time",
            x_title="time(HH:MM)",
            y_title="number of reads",
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", label_values(self.time_reads, self.x_labels))
        return plot

    def time_active_channels_plot(self):
        plot = pygal.Bar(
            title="Active channels over time",
            x_title="time(HH:MM)",
            y_title="active channels",
            style=ONE_SERIE_STYLE,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", label_values(self.time_active_channels, self.x_labels))
        return plot

    def time_quality_distribution_plot(self):
        plot = pygal.StackedBar(
            title="Quality distribution over time",
            style=QUALITY_DISTRIBUTION_STYLE,
            dots_size=1,
            y_labels=[i / 10 for i in range(11)],
            x_title="time(HH:MM)",
            y_title="fraction",
            fill=True,
            **label_settings(self.x_labels),
            **COMMON_GRAPH_OPTIONS,
        )
        for name, serie in zip(QUALITY_SERIES_NAMES, self.qual_percentages_over_time):
            serie_filled = sum(serie) > 0.0
            plot.add(name, label_values(serie, self.x_labels),
                     show_dots=serie_filled)
        return plot

    def channel_plot(self):
        plot = pygal.XY(
            title="Channel base yield and quality",
            dots_size=1,
            x_title="base yield (megabases)",
            y_title="quality (phred score)",
            stroke=False,
            style=ONE_SERIE_STYLE,
            **COMMON_GRAPH_OPTIONS
        )
        serie = []
        for channel, base_yield in self.per_channel_bases.items():
            quality = self.per_channel_quality[channel]
            serie.append(dict(value=(base_yield/1_000_000, quality),
                              label=str(channel)))
        plot.add(None, serie)
        return plot

    def translocation_section(self):
        transl_speeds = self.translocation_speed
        if sum(transl_speeds) == 0:
            return f"""
            {html_header("translocation speeds", 1)}
            Duration information not available.
            """
        too_slow = transl_speeds[:35] + [0] * 55
        too_fast = [0] * 45 + transl_speeds[45:]
        normal = [0] * 35 + transl_speeds[35:45] + [0] * 35
        total = sum(transl_speeds)
        within_bounds_frac = sum(normal) / total
        too_fast_frac = sum(too_fast) / total
        too_slow_frac = sum(too_slow) / total

        plot = pygal.Bar(
            title="Translocation speed distribution",
            x_title="Translocation_speed",
            y_title="active channels",
            style=pygal.style.DefaultStyle(
                # Use blue and red colors to accommodate colorblind people.
                colors=(QUALITY_COLORS[-3], QUALITY_COLORS[1], QUALITY_COLORS[1]),
                **COMMON_GRAPH_STYLE_OPTIONS,
            ),
            x_labels=[str(i) for i in range(0, 800, 10)] + [">800"],
            x_labels_major_every=10,
            show_minor_x_labels=False,
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("within bounds", normal)
        plot.add("too slow", too_slow)
        plot.add("too fast", too_fast)
        return f"""
        {html_header("translocation speeds", 1)}
        <p>Percentage of reads within accepted bounds: {within_bounds_frac:.2%}</p>
        <p>Percentage of reads that are too slow: {too_slow_frac:.2%}</p>
        <p>Percentage of reads that are too fast: {too_fast_frac:.2%}</p>
        {figurize_plot(plot)}
        """

    def to_html(self) -> str:
        if self.skipped_reason:
            return f"""
            {html_header("Nanopore time series", 1)}
            Skipped: {self.skipped_reason}
            """
        return f"""
        {html_header("Nanopore time series", 1)}
        {html_header("Base counts over time", 2)}
        {figurize_plot(self.time_bases_plot())}
        {html_header("Read counts over time", 2)}
        {figurize_plot(self.time_reads_plot())}
        {html_header("Active channels over time", 2)}
        {figurize_plot(self.time_active_channels_plot())}
        {html_header("Quality distribution over time", 2)}
        {figurize_plot(self.time_quality_distribution_plot())}
        {html_header("Per channel base yield versus quality", 2)}
        {figurize_plot(self.channel_plot())}
        {self.translocation_section()}
        """


@dataclasses.dataclass
class InsertSizeMetricsReport(ReportModule):

    insert_sizes: Sequence[int]

    @classmethod
    def from_insert_size_metrics(cls, metrics: InsertSizeMetrics):
        return cls(
            insert_sizes=metrics.insert_sizes().tolist(),
        )

    def insert_sizes_plot(self):
        insert_sizes_copy = list(self.insert_sizes)
        insert_sizes_copy[0] = 0
        plot = pygal.Bar(
            title="Insert Sizes",
            x_title="Insert size",
            y_title="Number of reads",
            style=ONE_SERIE_STYLE,
            x_labels=list(range(len(insert_sizes_copy))),
            x_labels_major_every=10,
            show_minor_x_labels=False,
            **COMMON_GRAPH_OPTIONS
        )
        plot.add("", insert_sizes_copy)
        return plot

    def to_html(self) -> str:
        total_reads = sum(self.insert_sizes)
        no_overlap_reads = self.insert_sizes[0]
        return f"""
            {html_header("Insert Sizes", 1)}
            <p class="explanation">
            Insert sizes are calculated by taking the first and last
            16&#8239;bp from read 2. These are searched for in read 1 while
            allowing at most 1 error. This method is very fast and reasonably
            accurate, but does not account for long poly-G cycles.
            </p>
            <table>
            <tr>
                <td>Total reads</td>
                <td style="text-align:right;">{total_reads:,}</td>
            </tr>
            <tr>
                <td>Reads without overlap</td>
                <td style="text-align:right;">{no_overlap_reads:,}</td>
                <td style="text-align:right;">
                    {no_overlap_reads / max(total_reads, 1):.2%}
                </td>
            </tr>
            </table>
            {figurize_plot(self.insert_sizes_plot())}
        """


@dataclasses.dataclass
class AdapterFromOverlapReport(ReportModule):
    total_reads: int
    number_of_adapters_read1: int
    number_of_adapters_read2: int
    adapters_read1: Sequence[Tuple[str, int]]
    adapters_read2: Sequence[Tuple[str, int]]
    longest_adapter_read1: str
    longest_adapter_read2: str
    longest_adapter_read1_match: str
    longest_adapter_read2_match: str

    @staticmethod
    def select_relevant_adapters(adapter_list: Sequence[Tuple[str, int]]):
        """
        Get the most prevalent adapters for each length. Sort the resulting
        list on length.
        """
        # Sort list on count
        sorted_list = sorted(adapter_list, reverse=True, key=lambda x: (x[1]))
        new_list = []
        lengths_to_get = set(range(1, INSERT_SIZE_MAX_ADAPTER_STORE_SIZE + 1))
        for adapter, count in sorted_list:
            if len(adapter) in lengths_to_get:
                lengths_to_get.remove(len(adapter))
                new_list.append((adapter, count))
        new_list.sort(key=lambda x: len(x[0]))
        return new_list

    @classmethod
    def from_insert_size_metrics(cls, metrics: InsertSizeMetrics):
        adapters_read1 = AdapterFromOverlapReport.select_relevant_adapters(
                metrics.adapters_read1())
        adapters_read2 = AdapterFromOverlapReport.select_relevant_adapters(
            metrics.adapters_read2())
        longest_adapter_read1 = adapters_read1[-1][0]
        longest_adapter_read2 = adapters_read2[-1][0]
        longest_adapter_read1_match = identify_sequence_builtin(
            longest_adapter_read1)[2]
        longest_adapter_read2_match = identify_sequence_builtin(
            longest_adapter_read2)[2]
        return cls(
            total_reads=metrics.total_reads,
            number_of_adapters_read1=metrics.number_of_adapters_read1,
            number_of_adapters_read2=metrics.number_of_adapters_read2,
            adapters_read1=adapters_read1,
            adapters_read2=adapters_read2,
            longest_adapter_read1=longest_adapter_read1,
            longest_adapter_read2=longest_adapter_read2,
            longest_adapter_read1_match=longest_adapter_read1_match,
            longest_adapter_read2_match=longest_adapter_read2_match,
        )

    def to_html(self) -> str:
        report = io.StringIO()
        report.write(f"""
            {html_header("Adapter Content", 1)}
            <p class="explanation">
                By calculating the overlap between reads it is possible to
                determine where the adapter is.
            </p>
            <table>
            <tr>
                <td>Total reads</td>
                <td style="text-align:right;">{self.total_reads:,}</td>
                <td style="text-align:right;">
                    {self.total_reads / max(self.total_reads, 1):.2%}
                </td>
            </tr>
            <tr>
                <td>Adapters in read 1</td>
                <td style="text-align:right;">{self.number_of_adapters_read1:,}</td>
                <td style="text-align:right;">
                    {self.number_of_adapters_read1 / max(self.total_reads, 1):.2%}
                </td>
            </tr>
            <tr>
                <td>Adapters in read 2</td>
                <td style="text-align:right;">{self.number_of_adapters_read2:,}</td>
                <td style="text-align:right;">
                    {self.number_of_adapters_read2 / max(self.total_reads, 1):.2%}
                </td>
            </tr>
            </table>
            <table><tr><th>Longest most frequent adapter</th>
            <th>Adapter Sequence</th><th>Best match</th></tr>
            <tr>
                <td>Read 1</td>
                <td>{self.longest_adapter_read1}</td>
                <td>{self.longest_adapter_read1_match}</td>
            </tr>
            <tr>
                <td>Read 2</td>
                <td>{self.longest_adapter_read2}</td>
                <td>{self.longest_adapter_read2_match}</td>
            </tr>
            </table>
        """)

        report.write(html_header("Adapters read 1", 2))
        report.write("<table>")
        report.write("<tr><th>Adapter</th><th>Count</th></tr>")
        for adapter, count in self.adapters_read1:
            report.write(
                f"""<tr>
                        <td>{adapter}</td>
                        <td style="text-align:right;">{count}</td>
                    </tr>
                """)
        report.write("</table>")

        report.write(html_header("Adapters read 2", 2))
        report.write("<table>")
        report.write("<tr><th>Adapter</th><th>Count</th></tr>")
        for adapter, count in self.adapters_read2:
            report.write(
                f"""<tr>
                        <td>{adapter}</td>
                        <td style="text-align:right;">{count}</td>
                    </tr>
                """)
        report.write("</table>")
        return report.getvalue()


NAME_TO_CLASS: Dict[str, Type[ReportModule]] = {
    "meta": Meta,
    "summary": Summary,
    "per_position_mean_quality_and_spread": PerPositionMeanQualityAndSpread,
    "per_position_quality_distribution": PerBaseQualityScoreDistribution,
    "sequence_length_distribution": SequenceLengthDistribution,
    "per_position_base_content": PerPositionBaseContent,
    "per_position_n_content": PerPositionNContent,
    "per_sequence_gc_content": PerSequenceGCContent,
    "per_sequence_quality_scores": PerSequenceAverageQualityScores,
    "adapter_content": AdapterContent,
    "per_tile_quality": PerTileQualityReport,
    "duplication_fractions": DuplicationCounts,
    "overrepresented_sequences": OverRepresentedSequences,
    "nanopore_metrics": NanoStatsReport,
    "adapter_content_from_overlap": AdapterFromOverlapReport,
    "insert_size_metrics": InsertSizeMetricsReport,
}

CLASS_TO_NAME: Dict[Type[ReportModule], str] = {
    value: key for key, value in NAME_TO_CLASS.items()}


def report_modules_to_dict(report_modules: Iterable[ReportModule]):
    def get_name(module: ReportModule) -> str:
        class_name = CLASS_TO_NAME[type(module)]
        if hasattr(module, "read_pair_info"):
            if module.read_pair_info == READ2:
                class_name += "_read2"
        return class_name
    return {
        get_name(module): module.to_dict()
        for module in report_modules
    }


def dict_to_report_modules(d: Dict[str, Dict[str, Any]]) -> List[ReportModule]:
    return [NAME_TO_CLASS[name.replace("_read2", "")].from_dict(
                NAME_TO_CLASS[name.replace("_read2", "")], class_dict)  # type: ignore
            for name, class_dict in d.items()]


def write_html_report(report_modules: Iterable[ReportModule],
                      html: str):
    for mod in report_modules:
        if isinstance(mod, Meta):
            filename = mod.filename
            break
    else:
        raise RuntimeError("No filename found in metadata")
    content_division = io.StringIO()
    content_division.write('<div class="content">')
    for module in report_modules:
        content_division.write(module.to_html())
    content_division.write("</div>")
    content = content_division.getvalue()
    toc = create_toc(content)

    with open(html, "wt", encoding="utf-8") as html_file:
        html_file.write(f"""
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="utf-8">
                <script>
                    {SEQUALI_REPORT_JS_CONTENT}
                </script>
                <script>
                    {SEQUALI_DOWNLOAD_SVG_JS_CONTENT}
                </script>
                <style>
                    {SEQUALI_REPORT_CSS_CONTENT}
                </style>
                <title>{os.path.basename(filename)}: Sequali Report</title>
            </head>
            <body>
        """)

        html_file.write(f"""
        <div class="toc">
        {html_header("Sequali report", 1)}
        <h2>Table of contents</h2>
        {toc}
        """)
        html_file.write("""
        <h2>Links</h2>
        <ul>
            <li><a href="https://sequali.readthedocs.io">Documentation</a></li>
            <li><a href="https://github.com/rhpvorderman/sequali"
                >Github repository</a></li>
            <li><a href="https://github.com/rhpvorderman/sequali/issues"
                >Bug tracker</a></li>
        </ul>
        """)
        html_file.write('</div>')
        html_file.write(content)
        html_file.write("</body></html>")


def qc_metrics_modules(metrics: QCMetrics,
                       data_ranges: Sequence[Tuple[int, int]],
                       read_pair_info: Optional[str] = None,
                       ) -> List[ReportModule]:
    base_count_tables = metrics.base_count_table()
    phred_count_table = metrics.phred_count_table()
    x_labels = stringify_ranges(data_ranges)
    aggregrated_base_matrix = aggregate_count_matrix(
        base_count_tables, data_ranges, NUMBER_OF_NUCS)
    aggregated_phred_matrix = aggregate_count_matrix(
        phred_count_table, data_ranges, NUMBER_OF_PHREDS)
    summary_bases = aggregate_count_matrix(
        aggregrated_base_matrix,
        [(0, len(aggregrated_base_matrix) // NUMBER_OF_NUCS)], NUMBER_OF_NUCS)
    summary_phreds = aggregate_count_matrix(
        aggregated_phred_matrix,
        [(0, len(aggregated_phred_matrix) // NUMBER_OF_PHREDS)],
        NUMBER_OF_PHREDS)
    total_bases = sum(summary_bases)
    minimum_length = 0
    total_reads = metrics.number_of_reads
    q20_reads = sum(metrics.phred_scores()[20:])
    for table in table_iterator(base_count_tables, NUMBER_OF_NUCS):
        if sum(table) < total_reads:
            break
        minimum_length += 1
    total_gc_bases = summary_bases[C] + summary_bases[G]
    return [
        Summary(
            mean_length=total_bases / max(total_reads, 1),
            minimum_length=minimum_length,
            maximum_length=metrics.max_length,
            total_reads=total_reads,
            total_bases=total_bases,
            q20_bases=sum(summary_phreds[5:]),
            q20_reads=q20_reads,
            total_gc_bases=total_gc_bases,
            total_n_bases=summary_bases[N],
            read_pair_info=read_pair_info),
        SequenceLengthDistribution.from_base_count_tables(
            base_count_tables, total_reads, data_ranges,
            read_pair_info=read_pair_info),
        PerBaseQualityScoreDistribution.from_phred_count_table_and_labels(
            aggregated_phred_matrix, x_labels, read_pair_info=read_pair_info),
        PerPositionMeanQualityAndSpread.from_phred_table_and_labels(
           aggregated_phred_matrix, x_labels, read_pair_info=read_pair_info),
        PerSequenceAverageQualityScores.from_qc_metrics(
            metrics, read_pair_info=read_pair_info),
        PerPositionBaseContent.from_base_count_tables_and_labels(
            aggregrated_base_matrix, x_labels, read_pair_info=read_pair_info),
        PerPositionNContent.from_base_count_tables_and_labels(
            aggregrated_base_matrix, x_labels, read_pair_info=read_pair_info),
        PerSequenceGCContent.from_qc_metrics(
            metrics, read_pair_info=read_pair_info),
    ]


def calculate_stats(
        filename: str,
        metrics: QCMetrics,
        per_tile_quality: PerTileQuality,
        sequence_duplication: SequenceDuplication,
        dedup_estimator: DedupEstimator,
        nanostats: NanoStats,
        adapters: List[Adapter],
        adapter_counter: Optional[AdapterCounter] = None,
        filename_reverse: Optional[str] = None,
        insert_size_metrics: Optional[InsertSizeMetrics] = None,
        metrics_reverse: Optional[QCMetrics] = None,
        per_tile_quality_reverse: Optional[PerTileQuality] = None,
        sequence_duplication_reverse: Optional[SequenceDuplication] = None,
        graph_resolution: int = 200,
        fraction_threshold: float = DEFAULT_FRACTION_THRESHOLD,
        min_threshold: int = DEFAULT_MIN_THRESHOLD,
        max_threshold: int = DEFAULT_MAX_THRESHOLD,
) -> List[ReportModule]:
    read_pair_info1 = READ1 if filename_reverse else None
    max_length = metrics.max_length
    if max_length > 500:
        data_ranges = list(logarithmic_ranges(max_length))
    else:
        data_ranges = list(equidistant_ranges(max_length, graph_resolution))
    modules = [
        Meta.from_filepath(filename, filename_reverse),
    ]
    # Generic modules for both read1 and read2 come first.
    if insert_size_metrics:
        modules.append(
            AdapterFromOverlapReport.from_insert_size_metrics(insert_size_metrics))
        modules.append(
            InsertSizeMetricsReport.from_insert_size_metrics(insert_size_metrics))
    if filename_reverse:
        modules.append(
            DuplicationCounts.from_dedup_estimator(dedup_estimator),
        )
    modules.extend(qc_metrics_modules(metrics, data_ranges,
                                      read_pair_info=read_pair_info1))
    if adapter_counter:
        modules.append(
            AdapterContent.from_adapter_counter_adapters_and_ranges(
                adapter_counter, adapters, data_ranges, read_pair_info=read_pair_info1)
        )
    modules.append(PerTileQualityReport.from_per_tile_quality_and_ranges(
        per_tile_quality, data_ranges, read_pair_info=read_pair_info1),)
    if not filename_reverse:
        modules.append(
            DuplicationCounts.from_dedup_estimator(dedup_estimator)
        )
    modules.append(
        OverRepresentedSequences.from_sequence_duplication(
            sequence_duplication,
            fraction_threshold=fraction_threshold,
            min_threshold=min_threshold,
            max_threshold=max_threshold,
            read_pair_info=read_pair_info1,
        )
    )

    if (metrics_reverse and per_tile_quality_reverse and
            sequence_duplication_reverse):
        max_length_reverse = metrics_reverse.max_length
        if max_length_reverse > 500:
            data_ranges_reverse = list(logarithmic_ranges(max_length_reverse))
        else:
            data_ranges_reverse = list(
                equidistant_ranges(max_length_reverse, graph_resolution))

        modules.extend(qc_metrics_modules(metrics_reverse, data_ranges_reverse,
                                          read_pair_info=READ2))
        modules.append(PerTileQualityReport.from_per_tile_quality_and_ranges(
            per_tile_quality_reverse, data_ranges_reverse,
            read_pair_info=READ2))
        modules.append(OverRepresentedSequences.from_sequence_duplication(
            sequence_duplication_reverse,
            fraction_threshold=fraction_threshold,
            min_threshold=min_threshold,
            max_threshold=max_threshold,
            read_pair_info=READ2,
        ))

    modules.append(NanoStatsReport.from_nanostats(nanostats))
    return modules
